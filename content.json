{"meta":{"title":"Misty Light","subtitle":"Thoughts about coding and life.","description":null,"author":"Misty Light","url":"https://mistylight.github.io","root":"/"},"pages":[{"title":"About Me","date":"2022-07-04T14:33:18.000Z","updated":"2023-06-05T03:53:09.620Z","comments":true,"path":"about/index.html","permalink":"https://mistylight.github.io/about/index.html","excerpt":"","text":"I’m a software engineer working in the bay area. During gradschool, I TA-ed for CS221: Artificial Intelligence: Principles and Techniques which cultivated my interest for tutorial writing (and, more importantly, paid my tuition fee 😃). I have a broad interest for computer science techniques (be it ML or non-ML), literature (bio, sci-fi, romance, poetry, etc.), and life hacks. This blog will be a collection of my thoughts on these topics. I started this blog to remind myself, that there are things driven by inexplicable impulses, that one shall cherish even if it’s unmonetizable. Just like the green light in the Great Gatsby, it’s a blessing to have an enchanted object that symbolizes one’s incorruptible dream. No matter how many times I get lost, tired, or even suffocated in the mist of reality, I’ll always try to reach my hand toward the distant light. I occasionally translate my articles to Chinese and publish them on zhihu (知乎)."},{"title":"Categories","date":"2019-08-14T01:07:11.000Z","updated":"2023-06-05T03:53:09.620Z","comments":false,"path":"categories/index.html","permalink":"https://mistylight.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2019-08-14T01:06:34.000Z","updated":"2023-06-05T03:53:09.620Z","comments":false,"path":"tags/index.html","permalink":"https://mistylight.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Understanding the EM Algorithm | Part II","slug":"en/Understanding the EM Algorithm - Part II","date":"2023-06-05T03:53:09.620Z","updated":"2021-08-27T00:00:00.000Z","comments":true,"path":"posts/16948/","link":"","permalink":"https://mistylight.github.io/posts/16948/","excerpt":"In the previous post, we case studied two examples: two coins and GMM, provided the algorithms for solving them (without proof), and derived a generalized form of the EM algorithm for solving a family of similar problems — finding the maximum log-likelihood with unknown hidden variables. This post will focus on the proof: Why are the algorithms showcased in the two examples mathematically equivalent to the final form of EM?","text":"In the previous post, we case studied two examples: two coins and GMM, provided the algorithms for solving them (without proof), and derived a generalized form of the EM algorithm for solving a family of similar problems — finding the maximum log-likelihood with unknown hidden variables. This post will focus on the proof: Why are the algorithms showcased in the two examples mathematically equivalent to the final form of EM? 💡 Algorithm 3: Expectation-Maximization (EM) — Final version Input: Observation {x(1)…x(n)}\\{x^{(1)}…x^{(n)}\\}{x(1)…x(n)}, initial guess for the parameter θ0\\theta_0θ0​ Output: Optimal parameter value θ∗\\theta^*θ∗ that maximizes the log-likelihoodL(θ)=∑i=1nlog⁡P(x(i);θ)\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\log P(x^{(i)};\\theta) L(θ)=i=1∑n​logP(x(i);θ) For t=1…Tt=1…Tt=1…T (until converge): E-Step: For each i=1…ni=1…ni=1…n, compute the hidden posterior:q(i)(z(i))=P(z(i)∣x(i);θt)q^{(i)}(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta_t) q(i)(z(i))=P(z(i)∣x(i);θt​) M-Step: Compute the maximizer for the evidence lower bound (ELBO):θt+1=arg⁡max⁡θ∑i=1nEz(i)∼q(i)(z(i))log⁡P(x(i),z(i); θ)\\theta_{t+1}=\\arg\\max_\\theta \\sum_{i=1}^n \\displaystyle \\mathop{\\mathbb{E}}_{z^{(i)}\\sim q^{(i)}(z^{(i)})}\\log P(x^{(i)}, z^{(i)};~ \\theta) θt+1​=argθmax​i=1∑n​Ez(i)∼q(i)(z(i))​logP(x(i),z(i); θ) Proof for two coins E-Step Remember that we computed the probability of a certain trial belonging to a certain coin in the E-step: q(1)(z(1))…q(5)(z(5))q^{(1)}(z^{(1)})…q^{(5)}(z^{(5)})q(1)(z(1))…q(5)(z(5)), where z(i)∈{A,B}z^{(i)} \\in \\{A, B\\}z(i)∈{A,B} is the coin tossed at the iii-th trial: Trial #1i=1i=1i=1 Trial #2i=2i=2i=2 Trial #3i=3i=3i=3 Trial #4i=4i=4i=4 Trial #5i=5i=5i=5 Observation: Number of heads x(i)x^{(i)}x(i) 5 9 8 4 7 Hidden posterior:q(i)(z(i)=A)q^{(i)}(z^{(i)}=A)q(i)(z(i)=A) = P(Trial is coin A | observation) 0.45 0.80 0.73 0.35 0.65 Hidden posterior:q(i)(z(i)=B)q^{(i)}(z^{(i)}=B)q(i)(z(i)=B) = P(Trial is coin B | observation) 0.55 0.20 0.27 0.65 0.35 Remember that this was computed using the posterior probability P(Trial is a certain coin | observation), which is consistent with Algorithm 3. M-Step It is slightly trickier to see why the “soft count” solution is equivalent to the M-step in Algorithm 3. To verify this, we expand the log part in the M-step formula as follows (Our derivation follows that of UMich EECS 545 lecture notes, adapted to our notion): θt+1=arg⁡max⁡θ∑i=1nEz(i)∼q(i)log⁡P(x(i),z(i);θ)=arg⁡max⁡θ∑i=1nEz(i)∼q(i)log⁡[P(z(i))⋅P(x(i)∣z(i);θ)]=arg⁡max⁡θ∑i=1nEz(i)∼q(i)[log⁡P(z(i))⏟constant wrt θcan be omitted+log⁡P(x(i)∣z(i);θ)]=arg⁡max⁡θ∑i=1nEz(i)∼q(i)log⁡P(x(i)∣z(i);θ)=arg⁡max⁡θ∑i=1nEz(i)∼q(i)log⁡[(10x(i))pz(i)x(i)(1−pz(i))(10−x(i))]=arg⁡max⁡θ∑i=1nEz(i)∼q(i)[log⁡(10x(i))⏟constant wrt θcan be omitted+x(i)log⁡pz(i)+(1−x(i))log⁡(10−pz(i))]=arg⁡max⁡θ∑i=1nEz(i)∼q(i)[x(i)log⁡pz(i)+(10−x(i))log⁡(1−pz(i))]=arg⁡max⁡θ∑i=1n{q(i)(z(i)=A)[x(i)log⁡pA+(10−x(i))log⁡(1−pA)]+ q(i)(z(i)=B)[x(i)log⁡pB+(10−x(i))log⁡(1−pB)]}=arg⁡max⁡θ∑i=1n{a(i)[x(i)log⁡pA+(10−x(i))log⁡(1−pA)]+ b(i)[x(i)log⁡pB+(10−x(i))log⁡(1−pB)]}\\begin{aligned} \\theta_{t+1}&amp;=\\textcolor{darkgrey}{\\arg\\max_\\theta \\sum_{i=1}^n \\displaystyle \\mathop{\\mathbb{E}}_{z^{(i)}\\sim q^{(i)}}} \\log P(x^{(i)}, z^{(i)}; \\theta)\\\\ &amp;= \\textcolor{darkgrey}{\\arg\\max_\\theta \\sum_{i=1}^n \\displaystyle \\mathop{\\mathbb{E}}_{z^{(i)}\\sim q^{(i)}}}\\log \\left[ P(z^{(i)})\\cdot P(x^{(i)}|z^{(i)};\\theta)\\right]\\\\ &amp;= \\textcolor{darkgrey}{\\arg\\max_\\theta \\sum_{i=1}^n \\displaystyle \\mathop{\\mathbb{E}}_{z^{(i)}\\sim q^{(i)}}}\\left[\\underbrace{\\log P(z^{(i)})}_{\\text{constant wrt }\\theta \\atop \\text{can be omitted}}+\\log P(x^{(i)}|z^{(i)};\\theta)\\right]\\\\ &amp;= \\textcolor{darkgrey}{\\arg\\max_\\theta \\sum_{i=1}^n \\displaystyle \\mathop{\\mathbb{E}}_{z^{(i)}\\sim q^{(i)}}} \\log P(x^{(i)}|z^{(i)};\\theta)\\\\ &amp;= \\textcolor{darkgrey}{\\arg\\max_\\theta \\sum_{i=1}^n \\displaystyle \\mathop{\\mathbb{E}}_{z^{(i)}\\sim q^{(i)}}} \\log\\left[ \\binom{10}{x^{(i)}} p_{z^{(i)}}^{x^{(i)}}(1-p_{z^{(i)}})^{(10-x^{(i)})}\\right]\\\\ &amp;= \\textcolor{darkgrey}{\\arg\\max_\\theta \\sum_{i=1}^n \\displaystyle \\mathop{\\mathbb{E}}_{z^{(i)}\\sim q^{(i)}}} \\left[ \\underbrace{\\log \\binom{10}{x^{(i)}}}_{\\text{constant wrt }\\theta \\atop \\text{can be omitted}}+x^{(i)}\\log p_{z^{(i)}}+(1-x^{(i)})\\log (10-p_{z^{(i)}}) \\right]\\\\ &amp;=\\textcolor{darkgrey}{\\arg\\max_\\theta \\sum_{i=1}^n \\displaystyle \\mathop{\\mathbb{E}}_{z^{(i)}\\sim q^{(i)}}} \\left[x^{(i)}\\log p_{z^{(i)}}+(10-x^{(i)})\\log (1-p_{z^{(i)}})\\right]\\\\ &amp;= \\textcolor{darkgrey}{\\arg\\max_\\theta \\sum_{i=1}^n }\\{\\textcolor{darkred}{q^{(i)}(z^{(i)}=A)}[x^{(i)}\\log p_A+(10-x^{(i)})\\log (1-p_A)]+\\\\&amp;~~~~~~~~~~~~~~~~~~~~~~~~~~~\\textcolor{green}{q^{(i)}(z^{(i)}=B)}[x^{(i)}\\log p_B+(10-x^{(i)})\\log (1-p_B)]\\}\\\\ &amp;= \\textcolor{darkgrey}{\\arg\\max_\\theta \\sum_{i=1}^n }\\{\\textcolor{darkred}{a^{(i)}}[x^{(i)}\\log p_A+(10-x^{(i)})\\log (1-p_A)]+\\\\&amp;~~~~~~~~~~~~~~~~~~~~~~~~~~~\\textcolor{green}{b^{(i)}}[x^{(i)}\\log p_B+(10-x^{(i)})\\log (1-p_B)]\\} \\end{aligned} θt+1​​=argθmax​i=1∑n​Ez(i)∼q(i)​logP(x(i),z(i);θ)=argθmax​i=1∑n​Ez(i)∼q(i)​log[P(z(i))⋅P(x(i)∣z(i);θ)]=argθmax​i=1∑n​Ez(i)∼q(i)​⎣⎢⎡​can be omittedconstant wrt θ​logP(z(i))​​+logP(x(i)∣z(i);θ)⎦⎥⎤​=argθmax​i=1∑n​Ez(i)∼q(i)​logP(x(i)∣z(i);θ)=argθmax​i=1∑n​Ez(i)∼q(i)​log[(x(i)10​)pz(i)x(i)​(1−pz(i)​)(10−x(i))]=argθmax​i=1∑n​Ez(i)∼q(i)​⎣⎢⎢⎢⎢⎡​can be omittedconstant wrt θ​log(x(i)10​)​​+x(i)logpz(i)​+(1−x(i))log(10−pz(i)​)⎦⎥⎥⎥⎥⎤​=argθmax​i=1∑n​Ez(i)∼q(i)​[x(i)logpz(i)​+(10−x(i))log(1−pz(i)​)]=argθmax​i=1∑n​{q(i)(z(i)=A)[x(i)logpA​+(10−x(i))log(1−pA​)]+ q(i)(z(i)=B)[x(i)logpB​+(10−x(i))log(1−pB​)]}=argθmax​i=1∑n​{a(i)[x(i)logpA​+(10−x(i))log(1−pA​)]+ b(i)[x(i)logpB​+(10−x(i))log(1−pB​)]}​ Taking the partial derivative with respect to θ={pA,pB}\\theta=\\{p_A, p_B\\}θ={pA​,pB​} setting it to zero, we have: ∂Eq[log⁡P(X,Z;θ)]∂pA=∑i=1na(i)x(i)pA−∑i=1na(i)(10−x(i))1−pA=0∂Eq[log⁡P(X,Z;θ)]∂pB=∑i=1nb(i)x(i)pB−∑i=1nb(i)(10−x(i))1−pB=0\\begin{aligned} \\dfrac{\\partial \\mathbb{E}_q[\\log P(X,Z;\\theta)]}{\\partial p_A} = \\dfrac{\\sum_{i=1}^na^{(i)}x^{(i)}}{p_A}-\\dfrac{\\sum_{i=1}^na^{(i)}(10-x^{(i)})}{1-p_A}=0\\\\ \\dfrac{\\partial \\mathbb{E}_q[\\log P(X,Z;\\theta)]}{\\partial p_B} = \\dfrac{\\sum_{i=1^n}b^{(i)}x^{(i)}}{p_B}-\\dfrac{\\sum_{i=1}^nb^{(i)}(10-x^{(i)})}{1-p_B}=0\\\\ \\end{aligned} ∂pA​∂Eq​[logP(X,Z;θ)]​=pA​∑i=1n​a(i)x(i)​−1−pA​∑i=1n​a(i)(10−x(i))​=0∂pB​∂Eq​[logP(X,Z;θ)]​=pB​∑i=1n​b(i)x(i)​−1−pB​∑i=1n​b(i)(10−x(i))​=0​ Solving this, we get: pA=∑i=1na(i)⋅x(i)∑i=1na(i)⋅10pB=∑i=1nb(i)⋅x(i)∑i=1nb(i)⋅10p_A=\\dfrac{\\sum_{i=1}^na^{(i)} \\cdot x^{(i)}}{\\sum_{i=1}^na^{(i)}\\cdot 10}\\\\ p_B=\\dfrac{\\sum_{i=1}^nb^{(i)} \\cdot x^{(i)}}{\\sum_{i=1}^nb^{(i)}\\cdot 10} pA​=∑i=1n​a(i)⋅10∑i=1n​a(i)⋅x(i)​pB​=∑i=1n​b(i)⋅10∑i=1n​b(i)⋅x(i)​ which is consistent with what we previously did using the “soft count”: pA(1)=0.45×5+0.80×9+0.73×8+0.35×4+0.65×7(0.45+0.80+0.73+0.35+0.65)×10≈0.713pB(1)=0.55×5+0.20×9+0.27×8+0.65×4+0.35×7(0.55+0.20+0.27+0.65+0.35)×10≈0.581\\begin{aligned} p_A^{(1)} &amp;= \\dfrac{\\textcolor{darkred}{0.45}\\times 5+\\textcolor{darkred}{0.80}\\times9+\\textcolor{darkred}{0.73}\\times8+\\textcolor{darkred}{0.35}\\times4+\\textcolor{darkred}{0.65}\\times 7}{(\\textcolor{darkred}{0.45+0.80+0.73+0.35+0.65})\\times10}\\approx 0.713\\\\\\\\ p_B^{(1)} &amp;= \\dfrac{\\textcolor{green}{0.55}\\times5+\\textcolor{green}{0.20}\\times9+\\textcolor{green}{0.27}\\times8+\\textcolor{green}{0.65}\\times4+\\textcolor{green}{0.35}\\times7}{(\\textcolor{green}{0.55+0.20+0.27+0.65+0.35})\\times 10}\\approx 0.581 \\end{aligned} pA(1)​pB(1)​​=(0.45+0.80+0.73+0.35+0.65)×100.45×5+0.80×9+0.73×8+0.35×4+0.65×7​≈0.713=(0.55+0.20+0.27+0.65+0.35)×100.55×5+0.20×9+0.27×8+0.65×4+0.35×7​≈0.581​ Therefore, our M-step is also consistent with Algorithm 3. Proof for GMM E-Step Similar to two coins, the hidden posterior was computed using the posterior probability P(Student is a certain gender | observation), which is consistent with Algorithm 3: Student #1i=1i=1i=1 Student #2i=2i=2i=2 Student #3i=3i=3i=3 Student #4i=4i=4i=4 Student #5i=5i=5i=5 Student #6i=6i=6i=6 Observation: Height x(i)x^{(i)}x(i) 168 180 170 172 178 176 Hidden Posterior:q(i)(z(i)=B)q^{(i)}(z^{(i)}=B)q(i)(z(i)=B) = P(boy | height) 0.26 1.00 0.50 0.74 0.99 0.96 Hidden Posterior:q(i)(z(i)=G)q^{(i)}(z^{(i)}=G)q(i)(z(i)=G) = P(girl | height) 0.74 0.00 0.50 0.26 0.01 0.04 M-Step Now let’s see why the weighted mean and standard deviation are the solution to the GMM M-step. I’m omitting the first few steps since they are identical to the two coins example: θt+1=arg⁡max⁡θ∑i=1nEz(i)∼q(i)log⁡P(x(i)∣z(i);θ)=arg⁡max⁡θ∑i=1nEz(i)∼q(i)log⁡N(x(i);μz(i),σz(i))=arg⁡max⁡θ∑i=1nEz(i)∼q(i)log⁡[1σz(i)2πexp⁡(−(x(i)−μz(i))22σz(i)2)]=arg⁡max⁡θ∑i=1nEz(i)∼q(i)[log⁡1σz(i)2π−(x(i)−μz(i))22σz(i)2]=arg⁡max⁡θ∑i=1n[q(i)(z(i)=B)(log⁡1σB2π−(x(i)−μB)22σB2)+q(i)(z(i)=G)(log⁡1σG2π−(x(i)−μG)22σG2)]=arg⁡max⁡θ∑i=1n[b(i)(log⁡1σB2π−(x(i)−μB)22σB2)+g(i)(log⁡1σG2π−(x(i)−μG)22σG2)]\\begin{aligned} \\theta_{t+1}&amp;= \\textcolor{darkgrey}{\\arg\\max_\\theta \\sum_{i=1}^n \\displaystyle \\mathop{\\mathbb{E}}_{z^{(i)}\\sim q^{(i)}}} \\log P(x^{(i)}|z^{(i)};\\theta)\\\\ % &amp;= \\textcolor{darkgrey}{\\arg\\max_\\theta \\sum_{i=1}^n \\displaystyle \\mathop{\\mathbb{E}}_{z^{(i)}\\sim q^{(i)}}} \\log \\mathcal{N}(x^{(i)}; \\mu_{z^{(i)}}, \\sigma_{z^{(i)}})\\\\ % &amp;= \\textcolor{darkgrey}{\\arg\\max_\\theta \\sum_{i=1}^n \\displaystyle \\mathop{\\mathbb{E}}_{z^{(i)}\\sim q^{(i)}}} \\log \\left[ \\dfrac{1}{\\sigma_{z^{(i)}}\\sqrt{2\\pi}} \\exp \\left( -\\dfrac{(x^{(i)}-\\mu_{z^{(i)}})^2}{2\\sigma_{z^{(i)}}^2} \\right) \\right]\\\\ % &amp;= \\textcolor{darkgrey}{\\arg\\max_\\theta \\sum_{i=1}^n \\displaystyle \\mathop{\\mathbb{E}}_{z^{(i)}\\sim q^{(i)}}} \\left[ \\log \\dfrac{1}{\\sigma_{z^{(i)}}\\sqrt{2\\pi}} - \\dfrac{(x^{(i)}-\\mu_{z^{(i)}})^2}{2\\sigma_{z^{(i)}}^2} \\right]\\\\ % &amp;= \\textcolor{darkgrey}{\\arg\\max_\\theta \\sum_{i=1}^n} \\left[ \\textcolor{darkred}{q^{(i)}(z^{(i)}=B)} \\left( \\log \\dfrac{1}{\\sigma_B\\sqrt{2\\pi}} - \\dfrac{(x^{(i)}-\\mu_B)^2}{2\\sigma_B^2} \\right) + \\textcolor{green}{q^{(i)}(z^{(i)}=G)} \\left( \\log \\dfrac{1}{\\sigma_G\\sqrt{2\\pi}} - \\dfrac{(x^{(i)}-\\mu_G)^2}{2\\sigma_G^2} \\right) \\right]\\\\ % &amp;= \\textcolor{darkgrey}{\\arg\\max_\\theta \\sum_{i=1}^n} \\left[ \\textcolor{darkred}{b^{(i)}} \\left( \\log \\dfrac{1}{\\sigma_B\\sqrt{2\\pi}} - \\dfrac{(x^{(i)}-\\mu_B)^2}{2\\sigma_B^2} \\right) + \\textcolor{green}{g^{(i)}} \\left( \\log \\dfrac{1}{\\sigma_G\\sqrt{2\\pi}} - \\dfrac{(x^{(i)}-\\mu_G)^2}{2\\sigma_G^2} \\right) \\right] % \\end{aligned} θt+1​​=argθmax​i=1∑n​Ez(i)∼q(i)​logP(x(i)∣z(i);θ)=argθmax​i=1∑n​Ez(i)∼q(i)​logN(x(i);μz(i)​,σz(i)​)=argθmax​i=1∑n​Ez(i)∼q(i)​log[σz(i)​2π​1​exp(−2σz(i)2​(x(i)−μz(i)​)2​)]=argθmax​i=1∑n​Ez(i)∼q(i)​[logσz(i)​2π​1​−2σz(i)2​(x(i)−μz(i)​)2​]=argθmax​i=1∑n​[q(i)(z(i)=B)(logσB​2π​1​−2σB2​(x(i)−μB​)2​)+q(i)(z(i)=G)(logσG​2π​1​−2σG2​(x(i)−μG​)2​)]=argθmax​i=1∑n​[b(i)(logσB​2π​1​−2σB2​(x(i)−μB​)2​)+g(i)(logσG​2π​1​−2σG2​(x(i)−μG​)2​)]​ Taking the partial derivative with respect to θ={μA,μB,σA,σB}\\theta=\\{\\mu_A, \\mu_B, \\sigma_A, \\sigma_B\\}θ={μA​,μB​,σA​,σB​} and setting it to zero, we have: ∂Eq[log⁡P(X,Z;θ)]∂μB=−∑i=1nb(i)(x(i)−μB)2σB2=0∂Eq[log⁡P(X,Z;θ)]∂μG=−∑i=1ng(i)(x(i)−μG)2σG2=0∂Eq[log⁡P(X,Z;θ)]∂σB=−∑i=1nb(i)σB+∑i=1nb(i)(x(i)−μB)2σB3=0∂Eq[log⁡P(X,Z;θ)]∂σG=−∑i=1ng(i)σG+∑i=1ng(i)(x(i)−μG)2σG3=0\\begin{aligned} % \\dfrac{\\partial \\mathbb{E}_q[\\log P(X,Z;\\theta)]}{\\partial \\mu_B} &amp;= -\\dfrac{\\sum_{i=1}^n b^{(i)}(x^{(i)}-\\mu_B)}{2\\sigma_B^2} =0\\\\ % \\dfrac{\\partial \\mathbb{E}_q[\\log P(X,Z;\\theta)]}{\\partial \\mu_G} &amp;= -\\dfrac{\\sum_{i=1}^n g^{(i)}(x^{(i)}-\\mu_G)}{2\\sigma_G^2} =0\\\\ % \\dfrac{\\partial \\mathbb{E}_q[\\log P(X,Z;\\theta)]}{\\partial \\sigma_B} &amp;= -\\dfrac{\\sum_{i=1}^n b^{(i)}}{\\sigma_B} + \\dfrac{\\sum_{i=1}^n b^{(i)}(x^{(i)}-\\mu_B)^2}{\\sigma_B^3} =0\\\\ % \\dfrac{\\partial \\mathbb{E}_q[\\log P(X,Z;\\theta)]}{\\partial \\sigma_G} &amp;= -\\dfrac{\\sum_{i=1}^n g^{(i)}}{\\sigma_G} + \\dfrac{\\sum_{i=1}^n g^{(i)}(x^{(i)}-\\mu_G)^2}{\\sigma_G^3} =0\\\\ % \\end{aligned} ∂μB​∂Eq​[logP(X,Z;θ)]​∂μG​∂Eq​[logP(X,Z;θ)]​∂σB​∂Eq​[logP(X,Z;θ)]​∂σG​∂Eq​[logP(X,Z;θ)]​​=−2σB2​∑i=1n​b(i)(x(i)−μB​)​=0=−2σG2​∑i=1n​g(i)(x(i)−μG​)​=0=−σB​∑i=1n​b(i)​+σB3​∑i=1n​b(i)(x(i)−μB​)2​=0=−σG​∑i=1n​g(i)​+σG3​∑i=1n​g(i)(x(i)−μG​)2​=0​ Solving these, we get: μB=∑i=1nb(i)x(i)∑i=1nb(i)μG=∑i=1ng(i)x(i)∑i=1ng(i)σB2=∑i=1nb(i)(x(i)−μB)2∑i=1nb(i)σG2=∑i=1ng(i)(x(i)−μG)2∑i=1ng(i)\\begin{aligned} % \\mu_B &amp;= \\dfrac{\\sum_{i=1}^n b^{(i)}x^{(i)}}{\\sum_{i=1}^n b^{(i)}}\\\\ % \\mu_G &amp;= \\dfrac{\\sum_{i=1}^n g^{(i)}x^{(i)}}{\\sum_{i=1}^n g^{(i)}}\\\\ % \\sigma_B^2 &amp;= \\dfrac{\\sum_{i=1}^n b^{(i)} (x^{(i)}-\\mu_B)^2}{\\sum_{i=1}^n b^{(i)}}\\\\ % \\sigma_G^2 &amp;= \\dfrac{\\sum_{i=1}^n g^{(i)} (x^{(i)}-\\mu_G)^2}{\\sum_{i=1}^n g^{(i)}} % \\end{aligned} μB​μG​σB2​σG2​​=∑i=1n​b(i)∑i=1n​b(i)x(i)​=∑i=1n​g(i)∑i=1n​g(i)x(i)​=∑i=1n​b(i)∑i=1n​b(i)(x(i)−μB​)2​=∑i=1n​g(i)∑i=1n​g(i)(x(i)−μG​)2​​ which is consistent with what we previously did using the “weighted” mean and standard deviation: μB(1)=0.26×168+1.00×180+...+0.96×1760.26+1.00+...+0.96≈175.5μG(1)=0.74×168+0.00×180+...+0.04×1760.74+0.00+...+0.04≈169.6σB(1)2=0.26×(168−175.5)2+...+0.96×(176−175.5)20.26+1.00+...+0.96≈3.83σG(1)2=0.74×(168−169.6)2+...+0.04×(176−169.6)20.74+0.00+...+0.04≈2.04\\begin{aligned} \\mu_B^{(1)} &amp;= \\dfrac{\\textcolor{darkred}{0.26}\\times168+\\textcolor{darkred}{1.00}\\times180+...+\\textcolor{darkred}{0.96}\\times176}{\\textcolor{darkred}{0.26+1.00+...+0.96}} \\approx 175.5\\\\\\\\ \\mu_G^{(1)} &amp;= \\dfrac{\\textcolor{green}{0.74}\\times168+\\textcolor{green}{0.00}\\times180+...+\\textcolor{green}{0.04}\\times176}{\\textcolor{green}{0.74+0.00+...+0.04}} \\approx 169.6\\\\\\\\ {\\sigma_B^{(1)}}^2 &amp;= \\dfrac{\\textcolor{darkred}{0.26}\\times(168-175.5)^2+...+\\textcolor{darkred}{0.96}\\times(176-175.5)^2}{\\textcolor{darkred}{0.26+1.00+...+0.96}}\\approx 3.83\\\\\\\\ {\\sigma_G^{(1)}}^2 &amp;= \\dfrac{\\textcolor{green}{0.74}\\times(168-169.6)^2+...+\\textcolor{green}{0.04}\\times(176-169.6)^2}{\\textcolor{green}{0.74+0.00+...+0.04}}\\approx 2.04 \\end{aligned} μB(1)​μG(1)​σB(1)​2σG(1)​2​=0.26+1.00+...+0.960.26×168+1.00×180+...+0.96×176​≈175.5=0.74+0.00+...+0.040.74×168+0.00×180+...+0.04×176​≈169.6=0.26+1.00+...+0.960.26×(168−175.5)2+...+0.96×(176−175.5)2​≈3.83=0.74+0.00+...+0.040.74×(168−169.6)2+...+0.04×(176−169.6)2​≈2.04​ Therefore, our M-step is also consistent with Algorithm 3. Closing Thoughts This wraps up my notes on the EM algorithm, and I hope you find it to be helpful! As I said at the beginning of Part I, EM is straightforward to understand through examples, yet tricky to fully understand the math behind. Therefore, I chose to put the examples and the ad-hoc algorithms first, and then prove that they are consistent with a more generalized theory. A fun fact is that I found Part II — while being math heavy — to be much easier to write than Part I, which focused on explaining the intuition. This doesn’t mean that the math is easy — it simply means that sometimes the intuition behind the math is extremely hard to put into words. I first learned the EM algorithm from Stanford CS229: Machine Learning, and then I became a course assistant for Stanford CS221: Artificial Intelligence: Principles and Techniques where I needed to explain this algorithm to many students. In CS229, I learned to navigate through the proof and use EM to solve several math problems, yet failed to develop a good intuition; In CS221, I was finally able to fully appreciate the beauty of this algorithm with a few “Explain Like I’m Five (ELI5)” examples similar to two coins — which I believe was not only helpful to myself, but also to quite a few CS221 students. Even so, I found it quite challenging to explain the intuition in words (rather than in math). After some research, I’ve found some excellent tutorials online, yet the majority of them are still overwhelmingly math heavy for beginners, while those with ELI5 examples usually skip the math derivations. Therefore, I decided to write a learning note by myself — with the hope that it will be accessible enough for beginners but still rigorous enough for readers seeking math proofs. Feel free to comment below or send me an email if you have any feedback or questions! References [1] What is the expectation maximization algorithm? Chuong B Do, Serafim Batzoglou. Nature, 2008. [paper] [2] Expectation Maximization. Benjamin Bray. UMich EECS 545: Machine Learning course notes, 2016. [course notes] [3] The EM algorithm. Tengyu Ma, Andrew Ng. Stanford CS 229: Machine Learning course notes, 2019. [course notes] [4] Bayesian networks: EM algorithm. Stanford CS 221: Artificial Intelligence: Principles and Techniques slides, 2021. [slides] [5] 如何感性地理解EM算法？工程师milter. 简书, 2017. [blog post] [6] Coin Flipping and EM. Karl Rosaen, chansoo. UMich EECS 545: Machine Learning Materials. [Jupyter Notebook]","categories":[{"name":"ML","slug":"ML","permalink":"https://mistylight.github.io/categories/ML/"},{"name":"Algorithms","slug":"ML/Algorithms","permalink":"https://mistylight.github.io/categories/ML/Algorithms/"}],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"https://mistylight.github.io/tags/tutorial/"},{"name":"understanding-X","slug":"understanding-X","permalink":"https://mistylight.github.io/tags/understanding-X/"},{"name":"EM","slug":"EM","permalink":"https://mistylight.github.io/tags/EM/"},{"name":"probability-theory","slug":"probability-theory","permalink":"https://mistylight.github.io/tags/probability-theory/"},{"name":"bayesian-network","slug":"bayesian-network","permalink":"https://mistylight.github.io/tags/bayesian-network/"}]},{"title":"Understanding the EM Algorithm | Part I","slug":"en/Understanding the EM Algorithm","date":"2023-06-05T03:53:09.620Z","updated":"2021-10-28T00:00:00.000Z","comments":true,"path":"posts/20115/","link":"","permalink":"https://mistylight.github.io/posts/20115/","excerpt":"The EM algorithm is very straightforward to understand with one or two proof-of-concept examples. However, if you really want to understand how it works, it may take a while to walk through the math. The purpose of this article is to establish a good intuition for you, while also provide the mathematical proofs for interested readers. The codes for all the examples mentioned in this article can be found at https://github.com/mistylight/Understanding_the_EM_Algorithm.","text":"The EM algorithm is very straightforward to understand with one or two proof-of-concept examples. However, if you really want to understand how it works, it may take a while to walk through the math. The purpose of this article is to establish a good intuition for you, while also provide the mathematical proofs for interested readers. The codes for all the examples mentioned in this article can be found at https://github.com/mistylight/Understanding_the_EM_Algorithm. Hello world: Two coins Let’s get started with a simple example from [1]. Warm-up Suppose you have 2 coins A and B with unknown probability of heads, pAp_ApA​ and pBp_BpB​. In order to estimate pAp_ApA​ and pBp_BpB​, you did an experiment consisting of 5 trials. In each trial, you pick either coin A or B, and then toss it for 10 times. Suppose this is what you get from your experiment: Trial ID Coin Result #heads / #tails #1 B HTTTHHTHTH 5/5 #2 A HHHHTHHHHH 9/1 #3 A HTHHHHHTHH 8/2 #4 B HTHTTTHHTT 4/6 #5 A THHHTHHHTH 7/3 How do we estimate pAp_ApA​ and pBp_BpB​ from this data? That’s quite straightforward. For coin A, it was tossed in trial #2, #3, #5, therefore we sum up the head count and divide them by the total number of tosses in these three trials: pA=9+8+710+10+10=0.8p_A=\\dfrac{9+8+7}{10+10+10}=0.8 pA​=10+10+109+8+7​=0.8 The same logic applies to coin B, which was tossed in trial #1 and #4: pB=5+410+10=0.45p_B = \\dfrac{5+4}{10+10} = 0.45 pB​=10+105+4​=0.45 To sum up, in order to infer pAp_ApA​ and pBp_BpB​ from a group of trials, we follow the two steps below: 💡 Algorithm 1: Maximum-Likelihood Estimation (MLE) — Infer pAp_ApA​ and pBp_BpB​ from complete data. Partition all trials into 2 groups: the ones with coin A, and the ones with coin B; Compute pAp_ApA​ as the total number of head tosses divided by the total number of tosses across the first group, and pBp_BpB​ as that of the second group. Challenge: From MLE to EM Now let’s consider a more challenging scenario: You forgot to take down which coin was tossed at each trial, so now your data looks like this: Trial ID Coin Result #heads / #tails #1 Unknown HTTTHHTHTH 5/5 #2 Unknown HHHHTHHHHH 9/1 #3 Unknown HTHHHHHTHH 8/2 #4 Unknown HTHTTTHHTT 4/6 #5 Unknown THHHTHHHTH 7/3 E-Step How do we estimate pAp_ApA​ and pBp_BpB​ from this incomplete data? The main problem is that the first step of Algorithm 1 no longer works — We cannot know for sure whether a trial belongs to coin A or coin B. The only intuition we can get here is that some of the trials are more likely to be coin A, while others are the opposite. For instance, since most of our trials have a head ratio of at least one half, one may conclude that one of the coins should have a probability of head slightly above 0.5 (let it be coin A), while the other is close to 0.5 (let it be coin B). Based on that assumption, trial #1 seems more likely to be coin B, while #2 seems more likely to be coin A. Is it possible to quantify that intuition with the language of probability? The answer is, yes and no: If we knew pAp_ApA​ and pBp_BpB​, the posterior probability of each trial being coin A or coin B could be calculated using the Bayes’ theorem. However, since we don’t know pAp_ApA​ and pBp_BpB​ (otherwise our problem would have been already solved!), this becomes a chicken-and-egg dilemma. The good news is that we can break the deadlock by giving an initial guess to pAp_ApA​ and pBp_BpB​ and gradually refine it afterwards. For instance, let’s guess pA(0)=0.6p_A^{(0)}=0.6pA(0)​=0.6, and pB(0)=0.5p_B^{(0)}=0.5pB(0)​=0.5. Given the tossing results, the probability of trial #2 being a certain coin can be calculated as: P(Coin A ∣ 9 heads+1 tail)∝P(Coin A,9 heads+1 tail)=12×(109)×0.69×(1−0.6)1≈0.020P(Coin B ∣ 9 heads+1 tail)∝P(Coin B,9 heads+1 tail)=12×(109)×0.59×(1−0.5)1≈0.005∴P(Coin A ∣ 9 heads+1 tail)=0.0200.020+0.005≈0.80P(Coin B ∣ 9 heads+1 tail)=0.0050.020+0.005≈0.20\\begin{aligned} P(\\text{Coin A} ~|~ \\text{9 heads+1 tail})&amp;\\propto P(\\text{Coin A}, \\text{9 heads+1 tail}) = \\dfrac{1}{2} \\times \\binom{10}{9} \\times 0.6^9 \\times (1-0.6)^1 \\approx 0.020\\\\ P(\\text{Coin B} ~|~ \\text{9 heads+1 tail})&amp;\\propto P(\\text{Coin B}, \\text{9 heads+1 tail}) = \\dfrac{1}{2} \\times \\binom{10}{9} \\times 0.5^9 \\times (1-0.5)^1 \\approx 0.005\\\\\\\\ \\therefore P(\\text{Coin A} ~|~ \\text{9 heads+1 tail}) &amp;= \\dfrac{0.020}{0.020+0.005} \\approx 0.80\\\\ P(\\text{Coin B} ~|~ \\text{9 heads+1 tail}) &amp;= \\dfrac{0.005}{0.020+0.005} \\approx 0.20 \\end{aligned} P(Coin A ∣ 9 heads+1 tail)P(Coin B ∣ 9 heads+1 tail)∴P(Coin A ∣ 9 heads+1 tail)P(Coin B ∣ 9 heads+1 tail)​∝P(Coin A,9 heads+1 tail)=21​×(910​)×0.69×(1−0.6)1≈0.020∝P(Coin B,9 heads+1 tail)=21​×(910​)×0.59×(1−0.5)1≈0.005=0.020+0.0050.020​≈0.80=0.020+0.0050.005​≈0.20​ Similarly, we can compute this probability for all 5 trials (E-step): Trial #1 Trial #2 Trial #3 Trial #4 Trial #5 P(Trial is coin A | trial result) 0.45 0.80 0.73 0.35 0.65 P(Trial is coin B | trial result) 0.55 0.20 0.27 0.65 0.35 M-Step Now looking back to Algorithm 1, we can make the following modification: Instead of using a “hard count” that counts each trial exclusively for either coin A or B, we now use a “soft count” that counts each trial partially for coin A, and partially for coin B. For instance, trial #2 (with 9 heads and 1 tail) has a 0.8 probability of being coin A, therefore it contributes a soft count of 9×0.8=7.29\\times 0.8=7.29×0.8=7.2 heads to coin A; It has a 0.2 probability of being coin B, therefore it contributes a soft count of 9×0.2=1.89\\times 0.2=1.89×0.2=1.8 heads to coin B. Similarly, we can compute the soft counts for all 5 trials, sum them up, and normalize to get the estimated probability for the two coins (M-step): pA(1)=0.45×5+0.80×9+0.73×8+0.35×4+0.65×7(0.45+0.80+0.73+0.35+0.65)×10≈0.713pB(1)=0.55×5+0.20×9+0.27×8+0.65×4+0.35×7(0.55+0.20+0.27+0.65+0.35)×10≈0.581\\begin{aligned} p_A^{(1)} &amp;= \\dfrac{\\textcolor{darkred}{0.45}\\times 5+\\textcolor{darkred}{0.80}\\times9+\\textcolor{darkred}{0.73}\\times8+\\textcolor{darkred}{0.35}\\times4+\\textcolor{darkred}{0.65}\\times 7}{(\\textcolor{darkred}{0.45+0.80+0.73+0.35+0.65})\\times10}\\approx 0.713\\\\\\\\ p_B^{(1)} &amp;= \\dfrac{\\textcolor{green}{0.55}\\times5+\\textcolor{green}{0.20}\\times9+\\textcolor{green}{0.27}\\times8+\\textcolor{green}{0.65}\\times4+\\textcolor{green}{0.35}\\times7}{(\\textcolor{green}{0.55+0.20+0.27+0.65+0.35})\\times 10}\\approx 0.581 \\end{aligned} pA(1)​pB(1)​​=(0.45+0.80+0.73+0.35+0.65)×100.45×5+0.80×9+0.73×8+0.35×4+0.65×7​≈0.713=(0.55+0.20+0.27+0.65+0.35)×100.55×5+0.20×9+0.27×8+0.65×4+0.35×7​≈0.581​ We will later see why this method makes sense mathematically. Note that our new estimation pA(1),pB(1)p_A^{(1)}, p_B^{(1)}pA(1)​,pB(1)​ are quite different from our original guess pA(0)=0.6,pB(0)=0.5p_A^{(0)}=0.6, p_B^{(0)}=0.5pA(0)​=0.6,pB(0)​=0.5. This indicates that our guess does not make the most sense, since otherwise we should have arrived at the same result. However, when we repeat the above procedure for 10 times, this gap shrinks and eventually converges to zero: Our final estimation for pA,pBp_A, p_BpA​,pB​ is p^A=0.797p^B=0.520\\hat p_A=0.797\\\\ \\hat p_B=0.520 p^​A​=0.797p^​B​=0.520 Note that this result matches our intuition: Comparing to our result in the warm-up scenario, due to the uncertainty in coin identity, our final estimation for coin A is slightly lower than 0.8 (as “pulled down” by coin B), and our estimation for coin B is slightly higher than 0.45 (as “pulled up” by coin A). To summarize, in order to infer pA,pBp_A, p_BpA​,pB​ from incomplete data with unknown coin identity at each trial, we follow the Expectation-Maximization (EM) algorithm as described below: 💡 Algorithm 2: Expectation-Maximization (EM) — Infer pAp_ApA​ and pBp_BpB​ from incomplete data. Repeat until converge: E-Step: Compute the probability of each trial being coin A or coin B; M-Step: Compute p^A,p^B\\hat p_A, \\hat p_Bp^​A​,p^​B​ as normalized soft count. Discussion The intuition makes sense, but why is this method mathematically correct? That is a very good question. In fact, it is worth noting that the “soft count” approach is an oversimplified version of the M-step that happens to be mathematically correct for this particular example (and the following GMM example). The general idea is that we can prove the likelihood function is monotonically non-decreasing after each iteration of E-step and M-step, so that our estimation for pA,pBp_A, p_BpA​,pB​ (or the “parameters”) gradually becomes better in the sense that it makes the observed coin tosses (or the “observation”) more likely to happen. I’m leaving the proof to the next post — check it out if you are interested! (warning: math ahead) Does EM always converge to the global optimum? It is not gauranteed that we will always arrive at the global optimum — quoting the Stanford CS229 course notes on the EM Algorithm, “Even for mixture of Gaussians, the EM algorithm can either converge to a global optimum or get stuck, depending on the properties of the training data.” That being said, in practice EM is considered to be a useful algorithm, as “Empirically, for real-world data, often EM can converge to a solution with relatively high likelihood (if not the optimum), and the theory behind it is still largely not understood.” Do we get different results if we started at a different initial guess? It is possible (though not gauranteed) that we still get the same result even if we started at a different initial guess. For instance, in the two coins example, a different guess pA(0)=0.7,pB(0)=0.4p_A^{(0)}=0.7, p_B^{(0)}=0.4pA(0)​=0.7,pB(0)​=0.4 produces the same results: If we plot the likelihood function L(pA,pB)=∑j=15log⁡[∑p∈{pA,pB}12C10HjpHj(1−p)10−Hj]\\mathcal{L}(p_A, p_B) = \\sum_{j=1}^5 \\log\\left[\\sum_{p\\in \\{p_A, p_B\\}}\\dfrac{1}{2}C_{10}^{H_j}p^{H_j}(1-p)^{10-H_j}\\right]L(pA​,pB​)=∑j=15​log[∑p∈{pA​,pB​}​21​C10Hj​​pHj​(1−p)10−Hj​] (where HjH_jHj​ is the number of head tosses in trial jjj) along with the intermediate results, it would become clear that both cases converge to the same optima through different optimization paths: pA(0)=0.6,pB(0)=0.5p_A^{(0)}=0.6, p_B^{(0)}=0.5pA(0)​=0.6,pB(0)​=0.5 pA(0)=0.7,pB(0)=0.4p_A^{(0)}=0.7, p_B^{(0)}=0.4pA(0)​=0.7,pB(0)​=0.4 GMM example: Girls and boys Now consider a new situation: Given 6 students whose heights are taken down in the following table, we’d like to infer the gender for each student (Note: in real life, inferring one’s gender based on their height might be a bad idea. This example is for educational purposes only). Student ID Gender Height (cm) #1 Unknown 168 #2 Unknown 180 #3 Unknown 170 #4 Unknown 172 #5 Unknown 178 #6 Unknown 176 For simplicity, we assume that the heights of the boys and the girls conform to normal distribution N(μB,σB)\\mathcal{N}(\\mu_B, \\sigma_B)N(μB​,σB​) and N(μG,σG)\\mathcal{N}(\\mu_G, \\sigma_G)N(μG​,σG​), respectively. Since the total distribution is the combination of two Gaussian distributions, this is called a Gaussian mixture model (GMM). The problem is similar to the two coins: In order to infer the gender of each student, we need to know the parameter θ={μB,μG,σB,σG}\\theta=\\{\\mu_B, \\mu_G, \\sigma_B, \\sigma_G\\}θ={μB​,μG​,σB​,σG​}; However, in order to estimate the parameter θ\\thetaθ, we need to know the gender of each student. Let’s see how the EM algorithm works in this scenario. E-Step Remember that the key idea of the E-step is to infer how likely it is that a certain data point (i.e. a student) belongs to a certain category (i.e. boy or girl). Remember also that we need an initial guess for the parameter θ\\thetaθ to kick start the computation, e.g. we may guess that the average height μB(0)=175\\mu_B^{(0)}=175μB(0)​=175 for boys and μG(0)=165\\mu_G^{(0)}=165μG(0)​=165 for girls, and the standard deviations σB(0)=σG(0)=4.32\\sigma_B^{(0)}= \\sigma_G^{(0)}=4.32σB(0)​=σG(0)​=4.32 (which is the standard deviation of the heights of all students). Under this setup, the probability of student #1 being a boy or a girl can be computed as: P(boy∣height=168cm)∝P(boy,height=168cm)=12⋅N(x=168; μ=175,σ=4.32)≈0.012P(girl∣height=168cm)∝P(girl,height=168cm)=12⋅N(x=168; μ=165,σ=4.32)≈0.036∴P(boy∣height=168cm)=0.0120.012+0.036≈0.26P(girl∣height=168cm)=0.0120.012+0.036≈0.74\\begin{aligned} P(\\text{boy} | \\text{height=168cm}) &amp;\\propto P(\\text{boy}, \\text{height=168cm}) =\\dfrac{1}{2}\\cdot \\mathcal{N}(x=168;~\\mu=175, \\sigma=4.32) \\approx 0.012\\\\ P(\\text{girl} | \\text{height=168cm}) &amp;\\propto P(\\text{girl}, \\text{height=168cm}) = \\dfrac{1}{2}\\cdot \\mathcal{N}(x=168;~\\mu=165, \\sigma=4.32) \\approx 0.036\\\\\\\\ \\therefore P(\\text{boy} | \\text{height=168cm}) &amp;= \\dfrac{0.012}{0.012+0.036}\\approx0.26\\\\ P(\\text{girl} | \\text{height=168cm}) &amp;= \\dfrac{0.012}{0.012+0.036}\\approx0.74 \\end{aligned} P(boy∣height=168cm)P(girl∣height=168cm)∴P(boy∣height=168cm)P(girl∣height=168cm)​∝P(boy,height=168cm)=21​⋅N(x=168; μ=175,σ=4.32)≈0.012∝P(girl,height=168cm)=21​⋅N(x=168; μ=165,σ=4.32)≈0.036=0.012+0.0360.012​≈0.26=0.012+0.0360.012​≈0.74​ Doing so for all 6 students gives us: Student #1 Student #2 Student #3 Student #4 Student #5 Student #6 P(boy | height) 0.26 1.00 0.50 0.74 0.99 0.96 P(girl | height) 0.74 0.00 0.50 0.26 0.01 0.04 M-Step Remember that the key idea of the M-step is to estimate the parameters by counting each data point partially for each category. Similar to the two coins example, we modify the equation for mean and standard deviation by weighting using the probability from the E-step: μB(1)=0.26×168+1.00×180+...+0.96×1760.26+1.00+...+0.96≈175.5μG(1)=0.74×168+0.00×180+...+0.04×1760.74+0.00+...+0.04≈169.6σB(1)2=0.26×(168−175.5)2+...+0.96×(176−175.5)20.26+1.00+...+0.96≈3.83σG(1)2=0.74×(168−169.6)2+...+0.04×(176−169.6)20.74+0.00+...+0.04≈2.04\\begin{aligned} \\mu_B^{(1)} &amp;= \\dfrac{\\textcolor{darkred}{0.26}\\times168+\\textcolor{darkred}{1.00}\\times180+...+\\textcolor{darkred}{0.96}\\times176}{\\textcolor{darkred}{0.26+1.00+...+0.96}} \\approx 175.5\\\\\\\\ \\mu_G^{(1)} &amp;= \\dfrac{\\textcolor{green}{0.74}\\times168+\\textcolor{green}{0.00}\\times180+...+\\textcolor{green}{0.04}\\times176}{\\textcolor{green}{0.74+0.00+...+0.04}} \\approx 169.6\\\\\\\\ {\\sigma_B^{(1)}}^2 &amp;= \\dfrac{\\textcolor{darkred}{0.26}\\times(168-175.5)^2+...+\\textcolor{darkred}{0.96}\\times(176-175.5)^2}{\\textcolor{darkred}{0.26+1.00+...+0.96}}\\approx 3.83\\\\\\\\ {\\sigma_G^{(1)}}^2 &amp;= \\dfrac{\\textcolor{green}{0.74}\\times(168-169.6)^2+...+\\textcolor{green}{0.04}\\times(176-169.6)^2}{\\textcolor{green}{0.74+0.00+...+0.04}}\\approx 2.04 \\end{aligned} μB(1)​μG(1)​σB(1)​2σG(1)​2​=0.26+1.00+...+0.960.26×168+1.00×180+...+0.96×176​≈175.5=0.74+0.00+...+0.040.74×168+0.00×180+...+0.04×176​≈169.6=0.26+1.00+...+0.960.26×(168−175.5)2+...+0.96×(176−175.5)2​≈3.83=0.74+0.00+...+0.040.74×(168−169.6)2+...+0.04×(176−169.6)2​≈2.04​ Repeating the E-step and the M-step for several iterations until convergence, we get the final answer: student #1, #3, #4 are most likely girls, while student #2, #5, #6 are most likely boys. We can also verify that the final values of μB,μG\\mu_B, \\mu_GμB​,μG​ are equal to the average heights of the male and the female students, respectively: Why EM works | Log-Likelihood and ELBO Now let’s dive into the math and answer the following questions — Why EM works? What is a more generalized form of the EM algorithm that can be applied to problems other than two coins and GMM? Log-Likelihood To begin with, given a dataset with nnn data points and unknown parameter θ\\thetaθ, the core problem that EM attempts to solve is to maximize the log-likelihood of the observed data: θ∗=arg⁡max⁡θ∑i=1nlog⁡P(x(i);θ)\\theta^* = \\arg\\max_\\theta\\sum_{i=1}^n \\log P(x^{(i)}; \\theta) θ∗=argθmax​i=1∑n​logP(x(i);θ) For instance, in the two coins example, the observation x(i)x^{(i)}x(i) is the coin tossing result at the iii-th trial, and the parameter θ={pA,pB}\\theta=\\{p_A, p_B\\}θ={pA​,pB​}; In the GMM example, the observation x(i)x^{(i)}x(i) is the height of the iii-th student, and the parameter θ={μB,μG,σB,σG}\\theta=\\{\\mu_B,\\mu_G,\\sigma_B,\\sigma_G\\}θ={μB​,μG​,σB​,σG​}. Hidden Variable Remember that in both examples, there’s missing information in the data, usually called a “hidden” variable, denoted as z(i)z^{(i)}z(i). For instance, in the two coins example, the hidden variable z(i)z^{(i)}z(i) is the coin identity at the iii-th trial; In the GMM example, z(i)z^{(i)}z(i) is the gender of the iii-th student. In order to compute the likelihood function, a common practice is to “break down” the likelihood into all categories by marginalizing over the hidden variable z(i)z^{(i)}z(i): For the sake of simplicity, we will consider the optimization for a single example xxx first, drop the outer sum and the superscript (i)^{(i)}(i), and add them back after we derived the algorithm. Our optimizaton target becomes: f(θ)=log⁡∑zP(x,z;θ)f(\\theta) = \\log\\sum_zP(x, z;\\theta) f(θ)=logz∑​P(x,z;θ) Note that we get a expression in the “log of sum” format. Quoting UMich EECS 545 lecture notes, in general this likelihood is non-convex with many local minima and hard to optimize: ELBO (Evidence Lower Bound) The good news is that we can construct a lower bound function g(θ)g(\\theta)g(θ) (usually called the ELBO, Evidence Lower Bound) — in the much nicer “sum of log” format, and use it as a proxy to optimize the log-likelihood function f(θ)f(\\theta)f(θ). In order to construct such a lower bound, one important, yet unintuitve trick is to introduce a new variable q(z)\\textcolor{darkblue}{q(z)}q(z) into the objective function, as it allows us to swap the order between log and sum. q(z)q(z)q(z) is an arbitrary probability distribution over zzz that satisfies ∑zq(z)=1\\sum_zq(z)=1∑z​q(z)=1. To put it into context, this is the probability distribution that we computed in the E-step, e.g. the probability that a certain trial belongs to coin A or B, or the probability that a certain student is male or female. We will later see that it should converge to the posterior distribution P(z∣x;θ∗)P(z|x; \\theta^*)P(z∣x;θ∗) as the algorithm iterates and converges to the optimal paramter θ∗\\theta^*θ∗: Note that it is possible for the inequality to become an equality: If we set q(z)=P(z∣x;θ)\\textcolor{darkblue}{q(z)}=P(z|x; \\theta)q(z)=P(z∣x;θ), the argument to log⁡(⋅)\\log(\\cdot)log(⋅) becomes P(x;θ)P(x;\\theta)P(x;θ), which is a constant value with respect to zzz — for which it does not matter whether the weighted average or the logarithm is applied first. Therefore, in that case the equality is satisified, and the ELBO ggg becomes a tight lower bound for the log-likelihood fff. Iterative Optimization But how can we optimize fff using its lower bound ggg? The answer is that we can do it in an iterative fashion (see the figure below): In the E-step, we “lift” the shape of ggg such that it reaches fff at the current θ\\thetaθ. This is achievable by setting q(z)=P(z∣x;θ)q(z)=P(z|x;\\theta)q(z)=P(z∣x;θ), as mentioned above; In the M-step, we find a new θ\\thetaθ that maximizes ggg, such that we arrive at a better solution than the beginning of this iteration. Convergence Formally, it can be proved that the log likelihood function fff is monotonically non-decreasing across the iterations: f(θt+1)≥⏟Log-likelihood ≥ ELBOg(θt+1,qt)≥⏟M-stepg(θt,qt)=⏟E-stepf(θt)f(\\theta_{t+1}) \\underbrace{\\geq}_{\\text{Log-likelihood }\\geq \\text{ ELBO}} g(\\theta_{t+1}, q_t) \\underbrace{\\geq}_{\\text{M-step}} g(\\theta_{t}, q_t)\\underbrace{=}_{\\text{E-step}}f(\\theta_t) f(θt+1​)Log-likelihood ≥ ELBO≥​​g(θt+1​,qt​)M-step≥​​g(θt​,qt​)E-step=​​f(θt​) This also proves that the EM algorithm will eventually converge (though the result is not guaranteed to be the global optimum). In order to visualize the convergence process, I constructed an example with the two coins problem: Suppose we keep pA=0.797p_A=0.797pA​=0.797 (which is already the optimal value) fixed and use EM to optimize pBp_BpB​ (initialized to pB(0)=0.2p_B^{(0)}=0.2pB(0)​=0.2), here is what we get in 3 EM iterations (Note how the ELBO is “lifted” in the E-step and maximized in the M-step): Simplifying the M-step Now if you are convinced that the EM algorithm will eventually converge to a point better than the initial guess, let’s move on to the actual computation part. Remember that in the M-step, we need to find the maximizer for the ELBO function, which looks quite complex: g(θ, q(z))=∑zq(z)log⁡P(x,z;θ)q(z)g(\\theta, ~\\textcolor{darkblue}{q(z)}) = \\sum_z q(z) \\log \\dfrac{P(x, z; \\theta)}{q(z)} g(θ, q(z))=z∑​q(z)logq(z)P(x,z;θ)​ It turns out that we can simplify this equation by expanding it and removing an irrelevant term: arg⁡max⁡θ∑zq(z)log⁡P(x,z;θ)q(z)=arg⁡max⁡θ∑zq(z)[log⁡P(x,z;θ)−log⁡q(z)]=arg⁡max⁡θ[∑zq(z)log⁡P(x,z;θ)−∑zq(z)log⁡q(z)⏟constant wrt θcan be ignored]=arg⁡max⁡θ∑zq(z)log⁡P(x,z;θ)=arg⁡max⁡θEz∼q(z)log⁡P(x,z;θ)\\begin{aligned} &amp;~~~~~\\arg\\max_\\theta \\sum_z q(z) \\log \\dfrac{P(x, z; \\theta)}{q(z)}\\\\ &amp;= \\arg\\max_\\theta \\sum_z q(z) \\left[\\log P(x, z; \\theta) - \\log q(z)\\right]\\\\ &amp;= \\arg\\max_\\theta [\\sum_z q(z) \\log P(x, z; \\theta) \\underbrace{-\\sum_z q(z)\\log q(z)}_{\\text{constant wrt } \\theta \\atop \\text{can be ignored}}]\\\\ &amp;=\\arg\\max_\\theta\\sum_z q(z) \\log P(x, z; \\theta)\\\\ &amp;= \\arg\\max_\\theta \\displaystyle \\mathop{\\mathbb{E}}_{z\\sim q(z)}\\log P(x, z; \\theta) \\end{aligned} ​ argθmax​z∑​q(z)logq(z)P(x,z;θ)​=argθmax​z∑​q(z)[logP(x,z;θ)−logq(z)]=argθmax​[z∑​q(z)logP(x,z;θ)can be ignoredconstant wrt θ​−z∑​q(z)logq(z)​​]=argθmax​z∑​q(z)logP(x,z;θ)=argθmax​Ez∼q(z)​logP(x,z;θ)​ To put it into words, this basically says “find the θ\\thetaθ that works the best in the average scenario”, where the “average” is weighted by a distribution q(z)q(z)q(z) — calculated in the E-step — that tells how likely it is that this particular data point xxx (e.g. a coin tossing trial) belongs to a certain category zzz (e.g. coin A or B) given the observation. Adding back the sum over all data points i=1…ni=1…ni=1…n, our final formula for the M-step becomes: θt+1=arg⁡max⁡θ∑i=1nEz(i)∼q(i)(z(i))log⁡P(x(i),z(i); θ)\\theta_{t+1}=\\arg\\max_\\theta \\sum_{i=1}^n \\displaystyle \\mathop{\\mathbb{E}}_{z^{(i)}\\sim q^{(i)}(z^{(i)})}\\log P(x^{(i)}, z^{(i)};~ \\theta) θt+1​=argθmax​i=1∑n​Ez(i)∼q(i)(z(i))​logP(x(i),z(i); θ) To summarize, a more generalized version of the EM algorithm looks like: 💡 Algorithm 3: Expectation-Maximization (EM) — Final version Input: Observation {x(1)…x(n)}\\{x^{(1)}…x^{(n)}\\}{x(1)…x(n)}, initial guess for the parameter θ0\\theta_0θ0​ Output: Optimal parameter value θ∗\\theta^*θ∗ that maximizes the log-likelihoodL(θ)=∑i=1nlog⁡P(x(i);θ)\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\log P(x^{(i)};\\theta) L(θ)=i=1∑n​logP(x(i);θ) For t=1…Tt=1…Tt=1…T (until converge): E-Step: For each i=1…ni=1…ni=1…n, compute the hidden posterior:q(i)(z(i))=P(z(i)∣x(i);θt)q^{(i)}(z^{(i)})=P(z^{(i)}|x^{(i)};\\theta_t) q(i)(z(i))=P(z(i)∣x(i);θt​) M-Step: Compute the maximizer for the evidence lower bound (ELBO):θt+1=arg⁡max⁡θ∑i=1nEz(i)∼q(i)(z(i))log⁡P(x(i),z(i); θ)\\theta_{t+1}=\\arg\\max_\\theta \\sum_{i=1}^n \\displaystyle \\mathop{\\mathbb{E}}_{z^{(i)}\\sim q^{(i)}(z^{(i)})}\\log P(x^{(i)}, z^{(i)};~ \\theta) θt+1​=argθmax​i=1∑n​Ez(i)∼q(i)(z(i))​logP(x(i),z(i); θ) Now you may wonder: How can the two coins example and the GMM example fit into this framework? The final form seems so complicated! In the next post, we will provide proofs that our previous methods used in these two examples are mathematically equivalent to Algorithm 3. You will probably be surprised that such simple and intuitive algorithms take so much to prove their correctness! References [1] What is the expectation maximization algorithm? Chuong B Do, Serafim Batzoglou. Nature, 2008. [paper] [2] Expectation Maximization. Benjamin Bray. UMich EECS 545: Machine Learning course notes, 2016. [course notes] [3] The EM algorithm. Tengyu Ma, Andrew Ng. Stanford CS 229: Machine Learning course notes, 2019. [course notes] [4] Bayesian networks: EM algorithm. Stanford CS 221: Artificial Intelligence: Principles and Techniques slides, 2021. [slides] [5] 如何感性地理解EM算法？工程师milter. 简书, 2017. [blog post] [6] Coin Flipping and EM. Karl Rosaen, chansoo. UMich EECS 545: Machine Learning Materials. [Jupyter Notebook]","categories":[{"name":"ML","slug":"ML","permalink":"https://mistylight.github.io/categories/ML/"},{"name":"Algorithms","slug":"ML/Algorithms","permalink":"https://mistylight.github.io/categories/ML/Algorithms/"}],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"https://mistylight.github.io/tags/tutorial/"},{"name":"understanding-X","slug":"understanding-X","permalink":"https://mistylight.github.io/tags/understanding-X/"},{"name":"EM","slug":"EM","permalink":"https://mistylight.github.io/tags/EM/"},{"name":"probability-theory","slug":"probability-theory","permalink":"https://mistylight.github.io/tags/probability-theory/"},{"name":"bayesian-network","slug":"bayesian-network","permalink":"https://mistylight.github.io/tags/bayesian-network/"},{"name":"simple-code-heavy-math","slug":"simple-code-heavy-math","permalink":"https://mistylight.github.io/tags/simple-code-heavy-math/"}]},{"title":"Understanding the VAE Model","slug":"en/Understanding the VAE Model","date":"2023-06-05T03:53:09.620Z","updated":"2022-02-24T00:00:00.000Z","comments":true,"path":"posts/10593/","link":"","permalink":"https://mistylight.github.io/posts/10593/","excerpt":"I believe most beginners to VAE (variational autoencoder) will encounter the following two types of introductions when searching for tutorials online, and may inevitably get a bit disoriented when they find the logical connections not very intuitive:","text":"I believe most beginners to VAE (variational autoencoder) will encounter the following two types of introductions when searching for tutorials online, and may inevitably get a bit disoriented when they find the logical connections not very intuitive: From a theoretical perspective, the VAE is a generative model that learns a probability distribution P(x)P(x)P(x) for datapoints that can be directly sampled from. It assumes that each of the datapoint xxx is generated according to the following process: (1) A latent variable zzz is sampled from a predefined prior distribution P(z)P(z)P(z), and then (2) A datapoint xxx is sampled from the conditional distribution P(x∣z)P(x|z)P(x∣z) (usually a neural network parametrized by learnable parameters θ\\thetaθ). The end goal is to maximize the dataset likelihood function ∑log⁡P(x)\\sum \\log P(x)∑logP(x). The graphical model of VAE. Image credit: [1] From a technical perspective, the VAE uses an encoder network to map each datapoint xxx to a distribution of its latent variable zzz, and then uses a decoder network to map it back to a distribution of its reconstruction x^\\hat xx^. The optimization goals are to (1) make x^\\hat xx^ as close to xxx as possible, and to (2) learn a meaningful latent space for zzz, such that we can generate novel samples by decoding from random noise. The encoder-decoder architecture of VAE. Image credit: The Fast Forward Labs blog This article aims to explain the math behind VAE and thereby clarify that the above two perspectives are in fact (almost) equivalent to each other. The codes for all the examples mentioned in this article can be found at https://github.com/mistylight/Understanding_the_VAE_Model. 📚 NOTE: This article assumes that the reader is familiar with the Expectation-Maximization (EM) algorithm. In case you are unfamiliar with EM, I have another tutorial on this topic: Understanding the EM Algorithm. Problem Statement Given a dataset X={x(1),x(2),...,x(n)}\\mathcal{X} = \\{x^{(1)}, x^{(2)}, ..., x^{(n)}\\}X={x(1),x(2),...,x(n)}, we assume that each of the datapoint x(i)x^{(i)}x(i) is generated by first sampling a latent variable z(i)z^{(i)}z(i) from a known prior distribution P(z)P(z)P(z), and then sampling x(i)x^{(i)}x(i) from a conditonal distribution P(x∣z(i);θ)P(x|z^{(i)};\\theta)P(x∣z(i);θ) parametrized by θ\\thetaθ. The ultimate goal is to find the optimal value of θ\\thetaθ that maximizes the dataset likelihood: θmax⁡=arg⁡max⁡θ∑i=1nlog⁡P(x(i);θ)\\theta_{\\max} = \\arg\\max_\\theta \\sum_{i=1}^n \\log P(x^{(i)}; \\theta) θmax​=argθmax​i=1∑n​logP(x(i);θ) Take the MNIST dataset as an example, one may think of x(i)x^{(i)}x(i) as an image containing one hand-written digit, and imagine z(i)z^{(i)}z(i) as a latent vector indicating the represented digit itself plus some style options such as the font family, font size, font weight, etc. In image generation, a common choice of zzz is the standard normal distribution N(0,1)\\mathcal{N}(0, 1)N(0,1), and usually P(x∣z(i);θ)P(x|z^{(i)};\\theta)P(x∣z(i);θ) is modeled as a neural network with learnable parameter θ\\thetaθ (e.g. A Bernoulli distribution with probability f(z;θ)f(z;\\theta)f(z;θ), where fff is a neural network taking zzz as an input and parametrized by θ\\thetaθ). One may wonder why it is valid to assume P(z)P(z)P(z) to be Gaussian. This is because it is provable that “any distribution in d dimensions can be generated by taking a set of d variables that are normally distributed and mapping them through a sufficiently complicated function” (cited from page 6 of [1]). Therefore, as long as P(x∣z;θ)P(x|z;\\theta)P(x∣z;θ) is a sufficiently powerful function approximator, e.g. a high-capacity neural network, then the resulting distribution of xxx can still be arbitrarily complex, which means the approximation error introduced by the assumption on P(z)P(z)P(z) is reasonably small. The Challenge with EM Now that we are presented with a max-likelihood problem with both unknown hidden variables and parameters, one may think of the EM algorithm as it is one of the most commonly used methods for solving problems of this kind. 💡 Algorithm 1: Expectation-Maximization (EM) Input: Observation {x(1)…x(n)}\\{x^{(1)}…x^{(n)}\\}{x(1)…x(n)}, initial guess for the parameter θ0\\theta_0θ0​ Output: Optimal parameter value θ∗\\theta^*θ∗ that maximizes the log-likelihoodL(θ)=∑i=1nlog⁡P(x(i);θ)\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\log P(x^{(i)};\\theta) L(θ)=i=1∑n​logP(x(i);θ) For t=1…Tt=1…Tt=1…T (until converge): E-Step: For each i=1…ni=1…ni=1…n, compute the hidden posterior:Q(i)(z(i))=P(z(i)∣x(i);θt)⏟intractableQ^{(i)}(z^{(i)})=\\textcolor{red}{\\underbrace{P(z^{(i)}|x^{(i)};\\theta_t)}_{\\text{intractable}}} Q(i)(z(i))=intractableP(z(i)∣x(i);θt​)​​ M-Step: Compute the maximizer for the evidence lower bound (ELBO):θt+1=arg⁡max⁡θ∑i=1nEz(i)∼Q(i)(z(i))log⁡P(x(i),z(i); θ)Q(i)(z)\\theta_{t+1}=\\arg\\max_\\theta \\sum_{i=1}^n \\displaystyle \\mathop{\\mathbb{E}}_{z^{(i)}\\sim Q^{(i)}(z^{(i)})}\\dfrac{\\log P(x^{(i)}, z^{(i)};~ \\theta)}{Q^{(i)}(z)} θt+1​=argθmax​i=1∑n​Ez(i)∼Q(i)(z(i))​Q(i)(z)logP(x(i),z(i); θ)​ Unfortunately, recall that the E-step of the EM algorithm requires that we compute the posterior distribution P(z∣x;θ)P(z|x;\\theta)P(z∣x;θ), which is usually intractable if zzz is a continuous variable and the conditional distribution P(x∣z;θ)P(x|z;\\theta)P(x∣z;θ) is defined as a neural network. This is because in order to compute the posterior, one needs to compute the integral over all possible values of zzz, which is intractable in practice (dropping the θ\\thetaθ for simplicity): P(z∣x)=P(x∣z)P(z)P(x)=P(x∣z)P(z)∫zP(x∣z)P(z)dzP(z|x) = \\dfrac{P(x|z)P(z)}{P(x)} = \\dfrac{P(x|z)P(z)}{\\int_z P(x|z)P(z) dz} P(z∣x)=P(x)P(x∣z)P(z)​=∫z​P(x∣z)P(z)dzP(x∣z)P(z)​ Variational Posterior Now that the problem is the intractability of computing P(z∣x;θ)P(z|x;\\theta)P(z∣x;θ) in the E-step, what if we borrow the idea from variational inference – introduce an approximate posterior Q(z∣x;ϕ)Q(z|x;\\phi)Q(z∣x;ϕ) which is tractable to compute, skip the E-step, and directly plug it into the M-step, where PPP and QQQ are optimized altogether? Modified EM: Use gradient descent to optimize ELBO w.r.t. both PPP and QQQ: θ∗,ϕ∗=arg⁡max⁡θ,ϕ∑i=1nEz(i)∼Q(z∣x(i);ϕ)log⁡P(x(i),z(i); θ)Q(z(i)∣x(i);ϕ)(∗)\\theta^*, \\phi^* = \\arg\\max_{\\theta, \\phi} \\sum_{i=1}^n \\displaystyle \\mathop{\\mathbb{E}}_{z^{(i)}\\sim Q(z|x^{(i)};\\phi)}\\dfrac{\\log P(x^{(i)}, z^{(i)};~ \\theta)}{Q(z^{(i)}|x^{(i)};\\phi)} \\quad (*) θ∗,ϕ∗=argθ,ϕmax​i=1∑n​Ez(i)∼Q(z∣x(i);ϕ)​Q(z(i)∣x(i);ϕ)logP(x(i),z(i); θ)​(∗) In practice, a common choice of Q(z∣x;ϕ)Q(z|x;\\phi)Q(z∣x;ϕ) is a Gaussian distribution N(μ(x;ϕ),σ(x;ϕ))\\mathcal{N}(\\mu(x;\\phi), \\sigma(x;\\phi))N(μ(x;ϕ),σ(x;ϕ)), where μ,σ\\mu, \\sigmaμ,σ are neural networks taking xxx as an input and parametrized by ϕ\\phiϕ. It has several nice properties, such as the ease of computation, which we will see later on. The Q(z∣x;ϕ)Q(z|x;\\phi)Q(z∣x;ϕ) network is also frequently called the “encoder” as it maps each datapoint xxx to a distribution of its corresponding latent variable zzz; The P(x∣z;ϕ)P(x|z;\\phi)P(x∣z;ϕ) network is commonly referred to as the “decoder” as it maps a given latent variable zzz to a distribution of reconstructed datapoint x^\\hat xx^. The encoder-decoder architecture of VAE. Image credit: The Fast Forward Labs blog Note that the main difference between (*) and the original EM algorithm is that: In the original EM algorithm, Q(z(i)∣x(i)):=P(z(i)∣x(i);θ)Q(z^{(i)}|x^{(i)}) := P(z^{(i)}|x^{(i)};\\theta)Q(z(i)∣x(i)):=P(z(i)∣x(i);θ) was precomputed in the E-step for every single datapoint in order to align the ELBO with the likelihood; In contrast, in the modified EM algorithm (*) Q(z∣x;ϕ)Q(z|x;\\phi)Q(z∣x;ϕ) is a neural network taking x(i)x^{(i)}x(i) as input, producing Q(z(i)∣x(i);ϕ)Q(z^{(i)}|x^{(i)};\\phi)Q(z(i)∣x(i);ϕ) as output, and jointly learned along with PPP. It is worth noticing that in this case, the optimization objective shifts from the dataset likelihood to the ELBO (Evidence Lower Bound), because we no longer get the correction from the E-step. This is an approximation to make the computation tractable. An example EM optimization process, note how the ELBO is &quot;aligned&quot; with the likelihood at the current value of θ in the E-step. This ensures that we are essentially optimizing the dataset likelihood instead of ELBO. In contrast, if we omit the E-step as discussed in this section, the optimization objective shifts from the likelihood to the ELBO. We&rsquo;ll quantify this error in the next section. The question here is: How much error are we introducing by omitting the E-step? Quantifying the Approximation Error To begin with, there are two types of errors when we optimize for a machine learning problem (see the Stanford CS221 course note for a more in-depth introduction): Approximation error: Errors stemming from the gap between the model family and the true distribution. In the case of VAE, this gap is caused by the omission of the E-step – the optimization objective has changed from the dataset likelihood (true objective) to ELBO (optimization proxy)! Estimation error: Errors caused by the inability to find the best model within the model family. In the case of VAE, this means the gap between the final ELBO and the optimal ELBO, and it usually stems from the imperfection of the stochastic gradient descent algorithm. Approximation error v.s. Estimation error. Figure credit: Stanford CS221 course note We are mostly interested in the approximation error – what is the best possible model we can get in this scenario? Recall the definition of ELBO for a single datapoint xxx and its corresponding latent variable zzz (here, we are dropping the superscriptions and θ,ϕ\\theta, \\phiθ,ϕ for clarity), which was optimized in the M-step of EM: ELBO(P,Q)=∫zQ(z∣x)log⁡P(x,z)Q(z∣x)dz=Ez∼Q(z∣x)[log⁡P(x,z)Q(z∣x)]\\text{ELBO}(P, Q) = \\int_z Q(z|x) \\log \\dfrac{P(x, z)}{Q(z|x)}dz = \\mathbb{E}_{z\\sim Q(z|x)}[\\log \\dfrac{P(x, z)}{Q(z|x)}] ELBO(P,Q)=∫z​Q(z∣x)logQ(z∣x)P(x,z)​dz=Ez∼Q(z∣x)​[logQ(z∣x)P(x,z)​] It is called the “evidence lower bound”, mainly because it is provable that it is a lower bound for the dataset likelihood, which is the true objective we wish to optimize: ELBO(P,Q)≤log⁡P(x)\\text{ELBO}(P, Q)\\leq\\log P(x) ELBO(P,Q)≤logP(x) The equality is satisfied if and only if Q(z∣x)≡P(z∣x)Q(z|x) \\equiv P(z|x)Q(z∣x)≡P(z∣x). Further, we can prove that the gap between ELBO and the dataset likelihood is exactly the KL divergence between the approximate posterior and the true posterior. Intuitively, this is exactly the error we introduced by omitting the E-step: ELBO(P,Q)=Ez∼Q(z∣x)[log⁡P(x,z)Q(z∣x)]‾=Ez∼Q(z∣x)[log⁡P(x,z)]‾−Ez∼Q(z∣x)[log⁡Q(z∣x)]=Ez∼Q(z∣x)[log⁡P(x)]‾+Ez∼Q(z∣x)[log⁡P(z∣x)]−Ez∼Q(z∣x)[log⁡Q(z∣x)]=log⁡P(x)+Ez∼Q(z∣x)[log⁡P(z∣x)]−Ez∼Q(z∣x)[log⁡Q(z∣x)]‾=log⁡P(x)−Ez∼Q(z∣x)[log⁡Q(z∣x)P(z∣x)]‾=log⁡P(x)−KL[Q(z∣x)∥P(z∣x)]⏟gap between ELBO and likelihood\\begin{aligned} &amp;\\quad\\text{ELBO}(P, Q) \\\\ &amp;= \\underline{\\mathbb{E}_{z\\sim Q(z|x)}[\\log \\dfrac{P(x, z)}{Q(z|x)}]} \\\\ &amp;= \\underline{\\mathbb{E}_{z\\sim Q(z|x)}[\\log P(x, z) ]} - \\mathbb{E}_{z\\sim Q(z|x)}[\\log Q(z|x) ] \\\\&amp;= \\underline{\\mathbb{E}_{z\\sim Q(z|x)}[\\log P(x) ]} + \\mathbb{E}_{z\\sim Q(z|x)}[\\log P(z|x) ] - \\mathbb{E}_{z\\sim Q(z|x)}[\\log Q(z|x) ] \\\\&amp;= \\log P(x) + \\underline{\\mathbb{E}_{z\\sim Q(z|x)}[\\log P(z|x) ] - \\mathbb{E}_{z\\sim Q(z|x)}[\\log Q(z|x) ]} \\\\&amp;= \\log P(x) - \\underline{\\mathbb{E}_{z\\sim Q(z|x)}[\\log\\dfrac{Q(z|x)}{P(z|x)}]} \\\\&amp;= \\log P(x) - \\underbrace{\\text{KL}[Q(z|x)\\| P(z|x)]}_{\\text{gap between ELBO and likelihood}} \\end{aligned}​ELBO(P,Q)=Ez∼Q(z∣x)​[logQ(z∣x)P(x,z)​]​=Ez∼Q(z∣x)​[logP(x,z)]​−Ez∼Q(z∣x)​[logQ(z∣x)]=Ez∼Q(z∣x)​[logP(x)]​+Ez∼Q(z∣x)​[logP(z∣x)]−Ez∼Q(z∣x)​[logQ(z∣x)]=logP(x)+Ez∼Q(z∣x)​[logP(z∣x)]−Ez∼Q(z∣x)​[logQ(z∣x)]​=logP(x)−Ez∼Q(z∣x)​[logP(z∣x)Q(z∣x)​]​=logP(x)−gap between ELBO and likelihoodKL[Q(z∣x)∥P(z∣x)]​​​ It turns out maximizing the ELBO is equivalent to maximizing the likelihood while minimizing the gap between the approximation posterior and the true posterior. It though remains questionable how much error is introduced by the last KL term as it deviates the optimization from the true likelihood. This deviation reflects the tradeoff between the accuracy v.s. tractability: At one extreme lies the EM algorithm, where the KL term is zeroed by setting Q(z∣x)≡P(z∣x)Q(z|x) \\equiv P(z|x)Q(z∣x)≡P(z∣x) at the cost of intractable integral computation; At the other extreme, if Q(z∣x)Q(z|x)Q(z∣x) is defined as a very low-capacity distribution that approximates P(z∣x)P(z|x)P(z∣x) very poorly, then the KL term becomes unignorable and the optimization objective will be completely off from the true likelihood. Quoting [1], we can have zero approximation error only if there exists a certain pair of (θ,ϕ)(\\theta, \\phi)(θ,ϕ) that maximizes log⁡P(x)\\log P(x)logP(x) AND satisfies P(z∣x)P(z|x)P(z∣x) being Gaussian for all x(i)x^{(i)}x(i)s. Does this optimal solution always exist? To the best of my knowledge, as of 2022, we cannot answer this question super well. There is though a body of research investigating the approximation error introduced by the KL term, either empirically or theoretically. For instance, [5] shows when both encoder and decoder are exponential families, the KL term can pull the models from the likelihood optimizers to a subset allowing the KL term to be zero, which can be detrimental to the performance if this subset is too restricted. [6] studies how is error influenced by various design choices, such as the capacity of the encoder network. In my view, [1] summarizes the status of this line of research quite well: “future theoretical work may show us how much approximation error VAEs have in more practical setups.” Implementing the Optimization As of this point, we’ve established that the VAE is kind of like a modified version of EM that compromises accuracy for tractability, and that as a result, it optimizes the ELBO which is similar to the dataset likelihood but not exactly the same. Let’s now focus on how to actually perform the optimization. The process involves some math tricks, but the ELBO eventually breaks down to two tractable objectives which are feasible to optimize with stochastic gradient descent: ELBO(P,Q)=Ez∼Q(z∣x)[log⁡P(x,z)Q(z∣x)]‾=Ez∼Q(z∣x)[log⁡P(x,z)]‾−Ez∼Q(z∣x)[log⁡Q(z∣x)]=Ez∼Q(z∣x)[log⁡P(x∣z)]+Ez∼Q(z∣x)[log⁡P(z)]−Ez∼Q(z∣x)[log⁡Q(z∣x)]‾=Ez∼Q(z∣x)[log⁡P(x∣z)]−Ez∼Q(z∣x)[Q(z∣x)P(z)]‾=Ez∼Q(z∣x)[log⁡P(x∣z)]⏟reconstruction−KL[Q(z∣x)∥P(z)]⏟latent space regularization\\begin{aligned} &amp;\\quad\\text{ELBO}(P, Q) \\\\ &amp;= \\underline{\\mathbb{E}_{z\\sim Q(z|x)}[\\log \\dfrac{P(x, z)}{Q(z|x)}]} \\\\ &amp;= \\underline{\\mathbb{E}_{z\\sim Q(z|x)}[\\log P(x, z) ]} - \\mathbb{E}_{z\\sim Q(z|x)}[\\log Q(z|x) ] \\\\&amp;= \\mathbb{E}_{z\\sim Q(z|x)}[\\log P(x|z) ] + \\underline{\\mathbb{E}_{z\\sim Q(z|x)}[\\log P(z) ] - \\mathbb{E}_{z\\sim Q(z|x)}[\\log Q(z|x) ]} \\\\&amp;= \\mathbb{E}_{z\\sim Q(z|x)}[\\log P(x|z) ] -\\underline{\\mathbb{E}_{z\\sim Q(z|x)} [\\dfrac{Q(z|x)}{P(z)}]} \\\\&amp;= \\underbrace{\\mathbb{E}_{z\\sim Q(z|x)}[\\log P(x|z) ]}_{\\text{reconstruction}} - \\underbrace{\\text{KL}[Q(z|x) \\| P(z)]}_{\\text{latent space regularization}} \\end{aligned} ​ELBO(P,Q)=Ez∼Q(z∣x)​[logQ(z∣x)P(x,z)​]​=Ez∼Q(z∣x)​[logP(x,z)]​−Ez∼Q(z∣x)​[logQ(z∣x)]=Ez∼Q(z∣x)​[logP(x∣z)]+Ez∼Q(z∣x)​[logP(z)]−Ez∼Q(z∣x)​[logQ(z∣x)]​=Ez∼Q(z∣x)​[logP(x∣z)]−Ez∼Q(z∣x)​[P(z)Q(z∣x)​]​=reconstructionEz∼Q(z∣x)​[logP(x∣z)]​​−latent space regularizationKL[Q(z∣x)∥P(z)]​​​ Intuitively, the first term encourages the reconstruction of the training set datapoints, for it maximizes the likelihood of training set datapoint xxx when the latent variable zzz is sampled from the encoder. This encourages the train set datapoints to be resampled from the decoder during training, therefore it’s usually referred to as the “reconstruction loss”; The second term regularizes the structure of the latent space of zzz, for it encourages the distribution of zzz values visited during training (produced by the encoder) to be as close to our predefined prior of zzz (usually N(0,1)\\mathcal{N}(0, 1)N(0,1)) as possible. This is especially useful if we wish to generate new datapoints by first sampling zzz from N(0,1)\\mathcal{N}(0, 1)N(0,1) and then sampling xxx from the decoder, because if there exist some values of zzz with high probability density in N(0,1)\\mathcal{N}(0, 1)N(0,1) (therefore making them very likely to be sampled) but rarely visited during training because the encoder assigns extremely low probability density to them, then it’d be very hard for the decoder to perform well on those unseen zzz values. The latent space of VAE with different loss functions. Image credit: Jeremy Jordan&rsquo;s blog Computing the Reconstruction Loss In stochastic gradient descent, the reconstruction loss Lrecon=Ez∼Q(z∣x)[log⁡P(x∣z)]\\mathcal{L}_{\\text{recon}} = \\mathbb{E}_{z\\sim Q(z|x)}[\\log P(x|z) ]Lrecon​=Ez∼Q(z∣x)​[logP(x∣z)] can be approximated by replacing the expectation operation with the batch mean as is the standard practice in deep learning: Lrecon≈1B∑i=1Blog⁡P(x(i)∣z(i);θ)\\mathcal{L_\\text{recon}} \\approx \\dfrac{1}{B}\\sum_{i=1}^B \\log P(x^{(i)}|z^{(i)};\\theta) Lrecon​≈B1​i=1∑B​logP(x(i)∣z(i);θ) Where, BBB is the batch size, x(i)x^{(i)}x(i) are datapoints from the train set, and z(i)z^{(i)}z(i) are sampled from Q(z∣x(i);ϕ)Q(z|x^{(i)};\\phi)Q(z∣x(i);ϕ). The tricky part is that the gradient cannot back propagate through the sampling operation, therefore it’d be hard to compute the gradient with respect to QQQ, i.e. ∇ϕLrecon\\nabla_\\phi \\mathcal{L}_{\\text{recon}}∇ϕ​Lrecon​. Fortunately, there is a math trick called “reparametrization” that solves this problem when QQQ is defined as a Gaussian distribution N(μ(x;ϕ),σ(x;ϕ))\\mathcal{N}(\\mu(x;\\phi), \\sigma(x;\\phi))N(μ(x;ϕ),σ(x;ϕ)): We can just sample ϵ(i)∼N(0,1)\\epsilon^{(i)} \\sim \\mathcal{N}(0, 1)ϵ(i)∼N(0,1) which is independent of QQQ and then z(i)=μ(x(i);ϕ)⋅ϵ(i)+σ(x(i);ϕ)z^{(i)} = \\mu(x^{(i)}; \\phi)\\cdot \\epsilon^{(i)} +\\sigma(x^{(i)};\\phi)z(i)=μ(x(i);ϕ)⋅ϵ(i)+σ(x(i);ϕ) would conform to exactly the same distribution as QQQ. This way we decouple the sampling procedure from the parameter optimization, and as a result we can now proceed with back propagation as normal: Lrecon≈1B∑i=1Blog⁡P(x(i)∣μ(x(i);ϕ)⋅ϵ(i)+σ(x(i);ϕ);θ)where ϵ(i)∼N(0,1)\\mathcal{L_\\text{recon}} \\approx \\dfrac{1}{B}\\sum_{i=1}^B \\log P(x^{(i)}\\mid\\mu(x^{(i)}; \\phi)\\cdot \\epsilon^{(i)} +\\sigma(x^{(i)};\\phi);\\theta)\\\\ \\text{where } \\epsilon^{(i)} \\sim \\mathcal{N}(0, 1) Lrecon​≈B1​i=1∑B​logP(x(i)∣μ(x(i);ϕ)⋅ϵ(i)+σ(x(i);ϕ);θ)where ϵ(i)∼N(0,1) When the training set datapoints x(i)x^{(i)}x(i)s are binary (e.g. black and white images), then if P(x∣z;θ)P(x|z;\\theta)P(x∣z;θ) is defined as the Bernoulli distribution, then the reconstruction loss is equivalent to the binary cross entropy (BCE) loss; Otherwise if P(x∣z;θ)P(x|z;\\theta)P(x∣z;θ) is defined as the Gaussian distribution, then the reconstruction loss is equivalent to a weighted L2 loss. Computing the KL Divergence Loss When both Q(z∣x;ϕ)Q(z|x;\\phi)Q(z∣x;ϕ) and P(z)P(z)P(z) are defined as a Gaussian distribution, The KL divergence loss LKL=KL[Q(z∣x)∥P(z)]\\mathcal{L}_{\\text{KL}} = \\text{KL}[Q(z|x) \\| P(z)]LKL​=KL[Q(z∣x)∥P(z)] can be straightforwardly computed as the KL divergence between two multivariate Gaussian distributions, which has a closed-form expression (the derivation can be found at this Stack Exchange thread): LKL=1B∑i=1B12[−∑j=1D(2log⁡σj+1)+∑j=1Dσj2+∑j=1Dμj2]\\mathcal{L}_{\\text{KL}} = \\dfrac{1}{B}\\sum_{i=1}^B\\dfrac{1}{2}[-\\sum_{j=1}^D (2\\log \\sigma_j+1) +\\sum_{j=1}^D\\sigma_j^2+\\sum_{j=1}^D\\mu_j^2]LKL​=B1​i=1∑B​21​[−j=1∑D​(2logσj​+1)+j=1∑D​σj2​+j=1∑D​μj2​] Where, BBB is the batch size, DDD is the dimension of zzz. iii indexes over the train set datapoints, jjj indexes over dimensions of zzz. σj,μj\\sigma_j, \\mu_jσj​,μj​ refer to the jjj-th dimension of the mean and the variance of Q(z∣x;ϕ)Q(z|x;\\phi)Q(z∣x;ϕ), respectively. To summarize: The encoder network Q(z∣x;ϕ)Q(z|x;\\phi)Q(z∣x;ϕ) and the decoder network P(x∣z;θ)P(x|z;\\theta)P(x∣z;θ) can be optimized end-to-end with loss function L=Lrecon+LKL\\mathcal{L} = \\mathcal{L}_{\\text{recon}}+\\mathcal{L}_{\\text{KL}}L=Lrecon​+LKL​ using stochastic gradient descent. Example: MNIST Now that we’ve clarified the structure and the optimization technique of VAE, let’s take a look at the tasks that VAE made possible. For this purpose, we’ll use the MNIST hand-written digit dataset and train a VAE network with CNN encoder and decoder architectures. My implementation can be found at https://github.com/mistylight/Understanding_the_VAE_Model. Latent Space If we map each datapoint xxx to its corresponding latent variable zzz with the encoder, and then project it to 2D with PCA, here is what we get: A visualization of the latent space of VAE trained on MNIST, projected to 2-dimension with PCA. There are two noticeable features: First, latents for the same digit tend to cluster together, while latents for different digits tend to occupy different regions of the space; Second, the latent space regions occupied by different digits gather together compactly, leaving almost no gap in between. The first feature allows for retrieving images with the same digit as a reference image, and the second feature allows for sampling from the latent space and generating a new image accordingly (otherwise our sample may fall into a “hole” of the latent space which is unseen during training!). This indicates that the VAE learns a meaningful internal representation for the data, which is one of the main reasons for its popularity. Reconstruction One may wonder how much information is preserved during the “compression” from the high dimensional xxx (a 28x28 image) to the low dimensional zzz (a 10D vector in our experiment). This compression happens at the connection between encoder and decoder, which is commonly referred to as the “bottleneck”. One way to look at this problem is to first map an input image xxx to its corresponding zzz with the encoder, and then map it back to a reconstructed image x^\\hat xx^ with the decoder. Here we show some pairs of (x,x^)(x, \\hat x)(x,x^): Input image x (left) and its corresponding VAE reconstruction x-hat (right). Note how the VAE preserves the identity of the digit and (almost) keeps the angle of the input. However, the details might be a bit off, and most noticeably the edges are always blurry. This indicates that the most important information of the input image is preserved but the finer details are mostly lost. We will also discuss the issue of blurriness in the FAQ section. Interpolation Remember the latent space is compact with almost no gap between. Then what if we start off from one point in the latent space and gradually travel to another point? What will the points in-between look like if we decode them into images? Interpolation between two points in the latent space. Here, we take two images xA,xBx_A, x_BxA​,xB​ from the test set of MNIST and sample 3 different points as the linear combination of their latent variables zA,zBz_A, z_BzA​,zB​, weighted by 25%, 50%, 75%, respectively. The results are as below: The interpolation between two test images xA, xB. From left to right: xA, xA-hat, the 3 interpolations in between, xB-hat, xB. It is interesting to see how the shape of the first digit gradually deforms to that of the second one. Note that if the path between the two latent variables crosses the latent space of a new digit (as is the case with the 1 -&gt; 3 interpolation in the first row, the path crosses the latent space of 6, as is shown in the latent space visualization), then we might see a new digit emerging during the interpolation. In practice, this interpolation may enable applications such as animation generation, which is very useful in the real world. Sample and Generate Wouldn’t it be cool if there was a button that randomly generates a brand new image every time you press it (as an ex-graphic designer, this has long been a dream of mine)? It turns out the VAE is capable of doing exactly this – you randomly sample a z∼N(0,1)z \\sim \\mathcal{N}(0, 1)z∼N(0,1), use the decoder to map it to image, and that’s it! Here are some samples of the generated image: Novel hand-written digit images generated by VAE. FAQ VAE vs AE? Before the invention of VAE, there is another family of models called the autoencoder (AE). The major difference between them is that the AE only optimizes for the reconstruction loss and doesn’t have the KL divergence loss to regularize the latent space. As a result, the AE doesn’t have the generative power of VAE, because when randomly sampling a latent variable from N(0,1)\\mathcal{N}(0, 1)N(0,1), there’s a great chance of falling into a “hole” unseen during training, resulting in poor decoding results. The “Visualization of latent space” of Jeremy Jordan’s blog has an excellent explanation for this. VAE vs GAN? In recent years, generative adversarial networks (GANs) have been increasingly popular and achieve the state-of-the-art performance in most image generation tasks. The images generated by GAN are usually much more plausible, because the GAN uses a higher-level loss function produced by a deep network (the discriminator) instead of the low-level BCE/L2 loss as the generation criterion. The downside is that GANs are usually harder to converge during training, and very often many tricks are combined together to stabilize the training and to improve the output quality. In other words, when a GAN isn’t converging, or is converging but produces very poor/almost identical samples, it’s generally harder to debug than a VAE. My two cents: If your goal is to achieve state-of-the-art results on a well-studied problem domain, e.g. image generation, then GAN is the way to go; Otherwise, if there are fewer references in the field, or if you are trying to generate something really complicated and less impaired by blurriness (e.g. 3D shapes, indoor scenes, layouts, etc), then VAE might be a better starting point, as they usually offer the first hint as to whether the problem itself is feasible at all. Why is VAE output blurry? There is more than one theory trying to explain this phenomenon, but this Reddit thread does a really good job describing one of them, so I’ll directly quote it without modification: This paper does a really good https://arxiv.org/pdf/1810.00597.pdf explanation of this phenomenon. To give a (crude) summary, it has to do with cases when you have different datapoints that have overlapping latent variables. The optimal reconstruction when you have no constraints like in vanilla VAEs is an average between those datapoints, resulting in the blurred sample (generally, averaging things in images makes them more “blurry”). The first figure does a good job of explaining this. There are solutions that can sidestep this problem by adding certain constraints to the optimization, so that you have better factorized latent representations (which often trades off with reconstruction accuracy). References [1] Tutorial on Variational Autoencoders. Doersch et al., 2016. https://arxiv.org/pdf/1606.05908.pdf [2] Auto-Encoding Variational Bayes. Kingma et al., 2014. https://arxiv.org/pdf/1312.6114.pdf [3] Variational inference. Stanford CS228. https://ermongroup.github.io/cs228-notes/inference/variational/ [4] Deriving the KL divergence loss for VAEs. Stack Exchange user user3658307 and Wei Zhong. 2020. https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes [5] VAE Approximation Error: ELBO and Exponential Families. Shekhovtsov et al., ICLR 2022. https://openreview.net/forum?id=OIs3SxU5Ynl [6] Inference Suboptimality in Variational Autoencoders. Cremer et al., ICML 2018. https://arxiv.org/pdf/1801.03558.pdf [7] Variational autoencoders. Jeremy Jordan, 2018. https://www.jeremyjordan.me/variational-autoencoders/ [8] 【Learning Notes】变分自编码器（Variational Auto-Encoder，VAE）. CSDN user MoussaTintin, 2016. https://blog.csdn.net/JackyTintin/article/details/53641885","categories":[{"name":"ML","slug":"ML","permalink":"https://mistylight.github.io/categories/ML/"},{"name":"Algorithms","slug":"ML/Algorithms","permalink":"https://mistylight.github.io/categories/ML/Algorithms/"}],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"https://mistylight.github.io/tags/tutorial/"},{"name":"understanding-X","slug":"understanding-X","permalink":"https://mistylight.github.io/tags/understanding-X/"},{"name":"probability-theory","slug":"probability-theory","permalink":"https://mistylight.github.io/tags/probability-theory/"},{"name":"bayesian-network","slug":"bayesian-network","permalink":"https://mistylight.github.io/tags/bayesian-network/"},{"name":"simple-code-heavy-math","slug":"simple-code-heavy-math","permalink":"https://mistylight.github.io/tags/simple-code-heavy-math/"},{"name":"VAE","slug":"VAE","permalink":"https://mistylight.github.io/tags/VAE/"},{"name":"variational-inference","slug":"variational-inference","permalink":"https://mistylight.github.io/tags/variational-inference/"},{"name":"deep-learning","slug":"deep-learning","permalink":"https://mistylight.github.io/tags/deep-learning/"},{"name":"neural-networks","slug":"neural-networks","permalink":"https://mistylight.github.io/tags/neural-networks/"}]},{"title":"Understanding the LDA Algorithm","slug":"en/Understanding the LDA Algorithm","date":"2023-06-05T03:53:09.620Z","updated":"2022-05-30T00:00:00.000Z","comments":true,"path":"posts/33996/","link":"","permalink":"https://mistylight.github.io/posts/33996/","excerpt":"The Latent Dirichlet Allocation (LDA) algorithm is a text mining algorithm that aims to extract topics from long texts. In a nutshell, LDA assumes that each document defines a distribution over topics, and each topic defines a distribution over words. Each word is generated by first sampling a topic from the document, and then sampling a word from the topic. To train an LDA is to solve for the parameters of these two distributions (doc-topic and topic-word) given many documents; To evaluate an LDA usually means predicting the topic distribution for a new unseen document.","text":"The Latent Dirichlet Allocation (LDA) algorithm is a text mining algorithm that aims to extract topics from long texts. In a nutshell, LDA assumes that each document defines a distribution over topics, and each topic defines a distribution over words. Each word is generated by first sampling a topic from the document, and then sampling a word from the topic. To train an LDA is to solve for the parameters of these two distributions (doc-topic and topic-word) given many documents; To evaluate an LDA usually means predicting the topic distribution for a new unseen document. This article will introduce LDA in a top-down fashion: Starting with the NeurIPS paper example, it first formulates topic modeling as a parameter estimation problem; then comes some essential math tools (e.g. Dirichlet distribution, Gibbs sampling, etc.); and finally, derives the LDA training and testing algorithms. 👉 Check out my LDA implementation if you are interested! https://github.com/mistylight/Understanding_the_LDA_Algorithm Example Imagine you are analyzing papers published in a machine learning conference. There are M=1000 accepted papers, and each paper has a N=200 word abstract. The conference has K=3 topics: vision, language, and graph. In order to analyze which topics each paper addresses, you make the following assumptions: Each paper defines a theme, which is a distribution over topics, e.g. 1&#123;Vision: 0.7, Language: 0.2, Graph: 0.1&#125; Each topic defines a distribution over words (same for all papers), e.g. 123Vision: &#123;&quot;image&quot;: 0.05, &quot;recognition&quot;: 0.01, &quot;detection&quot;: 0.01, ...&#125; Language: &#123;&quot;text&quot;: 0.07, &quot;semantic&quot;: 0.02, &quot;summarization&quot;: 0.01, ...&#125; Graph: &#123;&quot;node&quot;: 0.04, &quot;edge&quot;: 0.04, &quot;clustering&quot;: 0.01, ...&#125; The words in the papers are generated using the following procedure (as illustrated in the figure below): for m = 1…M // loop across documents Randomly generate a theme θ⃗\\vec\\thetaθ, e.g. &#123;Vision: 0.7, Language: 0.2, Speech: 0.1&#125; for n = 1…N // loop across words Randomly generate a topic zzz according to the theme, e.g. Vision Randomly generate a word www according to the topic, e.g. &quot;image&quot; Generating words in one paper according to the LDA model. Inspired by CS 221 course note [3] During LDA training, we are interested in solving the following 2 problems: Finding the topic distribution for each of the M existing papers (Θ∈RM×K\\mathbf \\Theta \\in \\mathbb{R}^{M\\times K}Θ∈RM×K); Finding the word distribution for each of the K topics (Φ∈RK×V\\mathbf \\Phi \\in \\mathbb{R}^{K\\times V}Φ∈RK×V, where V is the vocabulary size); We can then use these two matrices in LDA testing: Given a new unseen paper, find its topic distribution (θ⃗new∈RK\\vec\\theta_{\\text{new}} \\in \\mathbb{R}^Kθnew​∈RK). The Dirichlet distribution 101 Prior, data, and posterior Let’s take a closer look at the parameters we want to solve: the topic distribution for each of the M documents (Θ\\mathbf \\ThetaΘ) and the word distribution for each of the K topics (Φ\\mathbf \\PhiΦ). In order to estimate them, the LDA algorithm makes some assumptions about their prior. What is a prior? It’s our belief about the parameters without observing the data. In contrast, after you observe the data, you get the posterior. Usually we are interested in the posterior, since it incorporates the information of data and therefore is good for estimating the parameters using methods like MAP (maximum-a-posteriori) or EAP (expectated-a-posteriori). Here’s an example: Imagine you have a coin which you believe to be a fair coin, so you may think its probability of showing head is most likely around 0.5 (prior P(pcoin)P(p_{\\text{coin}})P(pcoin​)). However, after tossing it several times, it always shows head (data). Therefore your belief about its head probability gradually shifts from 0.5 to very close to 1 as the evidence accumulates (posterior P(pcoin∣data)P(p_{\\text{coin}}\\mid \\text{data})P(pcoin​∣data)). In this example, both the prior and the posterior of the coin’s probability can be nicely modeled with the Beta distribution. We can either report the expectation or the maximum of the posterior to answer the question “What do you think is the probability of coin after observing several tosses?” Example prior P(pcoin)P(p_{\\text{coin}})P(pcoin​) Example posterior P(pcoin∣data)P(p_{\\text{coin}}\\mid \\text{data})P(pcoin​∣data) after tossing and observing heads only Similarly, given a paper, we may have a prior that its topic distribution is most likely symmetric, e.g. &#123;Vision: 1/3, Language: 1/3, Graph: 1/3&#125; (prior P(θ⃗)P(\\vec\\theta)P(θ)). Similar to the coin, you may imagine this as a three-sided dice. After reading the paper through, you never encountered a single word about graphs, which means for 99% of the time you are seeing words like “image”, “captioning”, “text”, and so on (data W\\mathbf WW). At this moment your belief about its topic ratio (the probability of the dice) may shift towards something like &#123;Vision: 0.5, Language: 0.5, Graph: 0.0&#125; (posterior P(θ⃗∣W)P(\\vec\\theta \\mid \\mathbf{W})P(θ∣W)). In this example, both the prior and the posterior can be nicely modeled with the Dirichlet distribution. We can either report the expectation or the maximum of the posterior to answer the question “What do you think is the topic distribution of the paper after reading it through?” Example prior P(θ⃗)P(\\vec\\theta)P(θ) Example posterior P(θ⃗∣data)P(\\vec\\theta\\mid \\text{data})P(θ∣data) after observing only vision and language related words in the paper (figure credit) (figure credit) Note that in the above two examples, the Beta/Dirichlet distribution aren’t our only choice. We choose them mainly because they are mathematically convenient, for if the prior is Beta/Dirichlet, then the posterior is also Beta/Dirichlet. This algebraic convenience is frequently referred to as the “conjugate prior”. The Dirichlet Distribution: Jar of dices We’ve been using sloppy terms like “belief” to describe the following intuitions: A prior/posterior is a distribution that models the probability density of the estimated parameter. If we make the analogy that the topic distribution of a paper (a.k.a the “theme”) is like a K-sided dice, then the Dirichlet distribution is just like a jar of dices. Each time you reach in (which we call “sampling”), you get a new dice. In the paper example, before you observe the data, the jar is considered to be more likely to give you a fair dice; After you observe the data, the jar is considered to be more likely to give you a biased dice. The expectation of the posterior shifts as you accumulate more data. This is roughly equivalent to sampling from the jar many times and taking the average of the dices you get. Now let’s take a closer look at the Dirichlet distribution, our jar of dices. Recall that, a “dice” refers to a multinomial distribution Mult(p1,...,pn)\\text{Mult}(p_1,...,p_n)Mult(p1​,...,pn​) with ∑i=1npi=1\\sum_{i=1}^n p_i=1∑i=1n​pi​=1 and pi≥0p_i \\geq 0pi​≥0. A jar of dices is then a multivariate distribution over nnn variables p⃗=[p1,...,pn]\\vec{p} = [p_1, ...,p_n]p​=[p1​,...,pn​], where ∑pi=1\\sum p_i=1∑pi​=1 and pi≥0p_i \\geq 0pi​≥0. And remember our ultimate goal is to estimate the dice by solving for the posterior, which is the jar after observing some data. So, what is the data? Intuitively, the best way to collect data in order to estimate a dice, is to toss it many times and count the occurrences of each outcome. Here’s an example: Imagine you toss a three-sided doc-topic dice 100 times, and this is what you get: Vision: 50 (50%) Language: 40 (40%) Graph: 10 (10%) And suppose your prior is that the dice should be fair. The most naive way to incorporate this belief, is to add a “pseudo-count” to your observation. The amount of “pseudo-count” depends on the strength of your prior: If you really think the dice should be as fair as possible despite the observation, then you probably want to add a large pseudo-count (e.g. 100) to your data, which pulls the expectation of the posterior much closer to a fair dice: Vision: 50+100 (37.5%) Language: 40+100 (35%) Graph: 10+100 (27.5%) Otherwise you may want to add a tiny bit of pseudo-count (e.g. 1), which perturbs the observation by a small amount: Vision: 50+1 (49.5%) Language: 40+1 (39.8%) Graph: 10+1 (10.7%) And it turns out the Dirichlet distribution respects this pseudo-count intuition: It is parameterized by a parameter α⃗={α1,...,αn}\\vec{\\alpha}=\\{\\alpha_1,...,\\alpha_n\\}α={α1​,...,αn​}, where each αi\\alpha_iαi​ can be regarded as the pseudo-count for the iii-th outcome of the dice. We denote the Dirichlet distribution as P(p⃗;α⃗)P(\\vec{p};\\vec{\\alpha})P(p​;α), which means it’s a distribution with regard to the dice probabilities p⃗\\vec{p}p​ and parameterized by the pseudo-count α⃗\\vec\\alphaα. It has the following two nice properties: The Dirichlet posterior Dir(p⃗∣x⃗;α⃗)\\text{Dir}(\\vec{p}\\mid {\\vec{x}};\\vec\\alpha)Dir(p​∣x;α) after observing data x⃗=[x1,...,xn]\\vec{x}=[x_1,...,x_n]x=[x1​,...,xn​] (xix_ixi​ being the occurrences of the iii-th outcome) is a new Dirichlet distribution incorporating the prior’s pseudo-count with the observed data, which makes it straightforward to compute the posterior: Dir(p⃗∣x⃗;α⃗)=Dir(p⃗;α⃗+x⃗)\\text{Dir}(\\vec{p}\\mid \\vec{x}; \\vec{\\alpha}) = \\text{Dir}(\\vec{p}; \\vec{\\alpha}+\\vec x) Dir(p​∣x;α)=Dir(p​;α+x) The expectation of a Dirichlet prior Dir(p⃗;α⃗)\\text{Dir}(\\vec{p};\\vec{\\alpha})Dir(p​;α) is equal to the ratios between the αi\\alpha_iαi​s, which makes it straightforward to estimate the value of the parameter from a Dirichlet posterior: E[Dir(p⃗;α⃗)]=[α1∑i=1nαi,α2∑i=1nαi,...,αn∑i=1nαi]\\mathbb{E}[\\text{Dir}(\\vec{p}; \\vec{\\alpha})] = [\\dfrac{\\alpha_1}{\\sum_{i=1}^n \\alpha_i}, \\dfrac{\\alpha_2}{\\sum_{i=1}^n \\alpha_i},...,\\dfrac{\\alpha_n}{\\sum_{i=1}^n \\alpha_i}] E[Dir(p​;α)]=[∑i=1n​αi​α1​​,∑i=1n​αi​α2​​,...,∑i=1n​αi​αn​​] We are not going to dive into too much details, e.g. the definition of the Dirichlet distribution can be found here, and there are several interpretations for the intuition behind its density function [3]. However, to me I found it helpful to just regard it as a convenient math tool invented adhoc to model the jar of dices (i.e. the distribution of Multinomial distributions). And surprisingly, the LDA algorithm will not make use of its density function; A solid grasp of the above two properties is sufficient. Back to our problem Now going back to our problem: The parameters to estimate can be imagined as M doc-topic dices (each with K faces, as in Θ∈RM×K\\mathbf{\\Theta}\\in\\mathbb{R}^{M\\times K}Θ∈RM×K) and K topic-word dices (each with V faces, as in Φ∈RK×V\\mathbf{\\Phi}\\in\\mathbb{R}^{K\\times V}Φ∈RK×V​). The M doc-topic dices share the same prior, while the K topic-word dices share the other prior. Extending our analogy: Imagine there are 2 jars of dices, one jar filled with doc-topic dices, from which you randomly take M of them (Θ={θ1⃗,...,θM⃗}\\mathbf{\\Theta}=\\{\\vec{\\theta_1},...,\\vec{\\theta_M}\\}Θ={θ1​​,...,θM​​}, where each θ⃗m\\vec\\theta_mθm​ is a K-dim distribution over topics). These become the topic distributions of the M documents; Another jar filled with topic-word dices, from which you randomly take K of them (Φ={φ1⃗,...,φK⃗}\\mathbf{\\Phi}=\\{\\vec{\\varphi_1},...,\\vec{\\varphi_K}\\}Φ={φ1​​,...,φK​​}, where each φ⃗k\\vec\\varphi_kφ​k​ is a V-dim distribution over words). These become the word distributions of the K topics. Our goal is to estimate the values of Θ\\mathbf \\ThetaΘ and Φ\\mathbf \\PhiΦ by solving for their posteriors and taking the expectations: doc-topic dices Θ\\mathbf\\ThetaΘ topic-word dices Φ\\mathbf \\PhiΦ prior Dir(α⃗)\\text{Dir}(\\vec\\alpha)Dir(α), where α⃗\\vec\\alphaα is a hparam Dir(β⃗)\\text{Dir}(\\vec\\beta)Dir(β​), where β⃗\\vec\\betaβ​ is a hparam data The number of words with each topic (e.g. 100 words under Vision, 200 words under Language, 300 words under Graph) The occurrences of different words generated by the same topic (e.g. for topic Vision, “image” appeared 10 times, “recognition” appeared 20 times, etc.) posterior to be solved by LDA to be solved by LDA The LDA model Problem formulation Now refining the random sampling parts in our initial pseudo-code, we can summarize the LDA generative procedure as follows (see the figure below for an example): // initializing the M doc-topic dices and the K topic-word dices for m = 1…M // loop across documents Randomly sample a doc-topic dice θm⃗∼Dir(α⃗)\\vec{\\theta_m} \\sim \\text{Dir}(\\vec\\alpha)θm​​∼Dir(α) // K-dim probability distribution over all topics, // e.g. &#123;Vision: 0.7, Language: 0.2, Speech: 0.1&#125; for k = 1…K // loop across topics Randomly sample a topic-word dice φk⃗∼Dir(β⃗)\\vec{\\varphi_k} \\sim \\text{Dir}(\\vec\\beta)φk​​∼Dir(β​) // V-dim probability distribution over all words, // e.g. &#123;&quot;image&quot;: 0.05, &quot;recognition&quot;: 0.01, &quot;detection&quot;: 0.01, ...&#125; // Generate the words for m = 1…M // loop across documents for n = 1…N // loop across words Randomly sample a topic zmn∼Mult(θm⃗)z_{mn} \\sim \\text{Mult}(\\vec{\\theta_m})zmn​∼Mult(θm​​) // Integer in {1,...,K}\\{1,...,K\\}{1,...,K} representing a topic, e.g. Vision Randomly sample a word wmn∼Mult(φ⃗zmn)w_{mn} \\sim \\text{Mult}(\\vec{\\varphi}_{z_{mn}})wmn​∼Mult(φ​zmn​​) // Integer in {1,...,V}\\{1,...,V\\}{1,...,V} representing a word, e.g. &quot;image&quot; Our goals are as follows: For the train set documents, infer the latent variables Θ\\mathbf{\\Theta}Θ, Φ\\mathbf{\\Phi}Φ (i.e. the M doc-topic dices and the K topic-word dices) from the observed words W\\mathbf{W}W (The Dirichlet prior α⃗,β⃗\\vec\\alpha, \\vec\\betaα,β​ are hyperparameters of the LDA model) by taking the expectation of the posterior: Θ^=E[P(Θ∣W;α⃗,β⃗)]Φ^=E[P(Φ∣W;α⃗,β⃗)]\\begin{aligned} \\hat{\\mathbf{\\Theta}} &amp;= \\mathbb{E}[P(\\mathbf{\\Theta} \\mid \\mathbf{W} ; \\vec\\alpha, \\vec\\beta)]\\\\ \\hat{\\mathbf{\\Phi}} &amp;= \\mathbb{E}[P(\\mathbf{\\Phi} \\mid \\mathbf{W} ; \\vec\\alpha, \\vec\\beta)] \\end{aligned} Θ^Φ^​=E[P(Θ∣W;α,β​)]=E[P(Φ∣W;α,β​)]​ For an unseen test document with words Wnew\\mathbf{W}_{\\text{new}}Wnew​, infer its latent variable θ⃗new\\vec\\theta_{\\text{new}}θnew​ (remember we sample a new doc-topic dice for each document, but reuse the topic-word dices for all documents. Therefore we only need to infer a new θ⃗\\vec\\thetaθ, not φ⃗\\vec\\varphiφ​). This can be achieved by computing the expectation of the posterior: θnew⃗^=E[P(θ⃗new∣W∪Wnew;α⃗,β⃗)]\\hat{\\vec{\\theta_{\\text{new}}}} = \\mathbb{E}[P(\\vec\\theta_{\\text{new}}\\mid \\mathbf{W}\\cup\\mathbf{W_{\\text{new}}} ; \\vec\\alpha, \\vec\\beta)] θnew​​^​=E[P(θnew​∣W∪Wnew​;α,β​)] The hidden variable Z How do we estimate the posterior of Θ\\mathbf\\ThetaΘ and Φ\\mathbf\\PhiΦ? Well, remember we discussed that it’s straightforward to compute the Dirichlet posteriors as long as we have the data handy: doc-topic dices Θ\\mathbf\\ThetaΘ topic-word dices Φ\\mathbf \\PhiΦ prior Dir(α⃗)\\text{Dir}(\\vec\\alpha)Dir(α), where α⃗\\vec\\alphaα is a hparam Dir(β⃗)\\text{Dir}(\\vec\\beta)Dir(β​), where β⃗\\vec\\betaβ​ is a hparam data The number of words with each topic (e.g. 100 words under Vision, 200 words under Language, 300 words under Graph) The occurrences of different words generated by the same topic (e.g. for topic Vision, “image” appeared 10 times, “recognition” appeared 20 times, etc.) posterior to be solved by LDA to be solved by LDA The problem is that we don’t really know which topic is behind each word! If you are familiar with the EM algorithm, you’d probably know this is called a “hidden variable”. Here, the topic variable Z∈RM×N\\mathbf Z\\in\\mathbb{R}^{M\\times N}Z∈RM×N (One topic for each of the N words in each of the M documents) is crucial for computing the posterior (see the table above), yet it’s unknown to us. One way to bypass this challenge, is to sample the value of Z\\mathbf ZZ given the words W∈RM×N\\mathbf W \\in \\mathbb{R}^{M\\times N}W∈RM×N: Z∼P(Z∣W)\\mathbf Z \\sim P(\\mathbf Z \\mid \\mathbf W) Z∼P(Z∣W) The intuition here is that we want to know which topics are most likely given the words we see, and thus using them to estimate the parameters would make the most sense. And here we make the approximation of using one sample of Z\\mathbf ZZ to represent the average scenario of all possible Z\\mathbf ZZs. Nowadays, this is a strategy widely adopted by neural networks with stochastic components (e.g. VAEs). Mathematically, we can dive deeper to see why sampling Z\\mathbf ZZ from P(Z∣W)P(\\mathbf Z \\mid \\mathbf W)P(Z∣W) makes sense. The reader may refer to the appendix for a more in-depth explanation (warning: math ahead). Gibbs Sampling But how do you generate samples from a distribution? Simple as it may seem, this isn’t a trivial question. That being said, there are many well-developed algorithms that does exactly this. Gibbs sampling is the one we will use here because it’s mathematically convenient under the LDA framework. We’ll introduce the algorithm here without explaining why it works, but interested readers may refer to this tutorial (or: section 0.4 of [3] if you read Chinese). Gibbs sampling in 2D. Figure credit: Jessica Stringham's blog. Algorithm: Gibbs sampling Input: The math equation of p(x⃗)p(\\vec x)p(x). That is, for any given x⃗\\vec xx, we know how to compute its probability; This, however, doesn’t always make it feasible to compute statistics like expecatation/variances, which usually requires an integral and can be intractable in practice. Output: Samples {x⃗(1),x⃗(2),...,x⃗(T)}\\{\\vec x^{(1)},\\vec x^{(2)},...,\\vec x^{(T)}\\}{x(1),x(2),...,x(T)} drawn from p(x⃗)p(\\vec x)p(x). We can then make use of these samples to approximate the statistics we are interested in, e.g. the expectation as the average across all samples. Sampling from a probability distribution. // random initialization for x⃗(0)∈RD\\vec x^{(0)} \\in \\mathbb{R}^Dx(0)∈RD [x1(0),x2(0),...,xD(0)]←random vector[x^{(0)}_1,x_2^{(0)},...,x_D^{(0)}]\\leftarrow \\text{random vector}[x1(0)​,x2(0)​,...,xD(0)​]←random vector // iterative sampling for t in 0…T // repeat while not converged for d in 1…dim(x⃗\\vec xx): // loop through the dimensions of x⃗\\vec xx sample xd(t+1)∼P(⋅∣x1(t+1),...xd−1(t+1),xd+1(t),..,xD(t))(∗)x^{(t+1)}_d \\sim P(\\cdot \\mid x_1^{(t+1)},...x_{d-1}^{(t+1)},x_{d+1}^{(t)},..,x_D^{(t)}) \\quad (*)xd(t+1)​∼P(⋅∣x1(t+1)​,...xd−1(t+1)​,xd+1(t)​,..,xD(t)​)(∗) output x⃗(t+1)\\vec x^{(t+1)}x(t+1) as a new sample In one sentence, what (*) does is: Freeze the values of all other dimensions, and sample the current dimension according to their values. For instance, in 2D, Gibbs sampling alternates between sampling xxx while freezing yyy, and sampling yyy while freezing xxx: Gibbs sampling path in 2D. Figure credit: Jessica Stringham's blog. And in the 3D scenario, each Gibbs sampling iteration (i.e. the inner loop) is equivalent to: x1(t+1)∼P(⋅∣x2(t),x3(t)) // freeze x2,x3, only sample x1x2(t+1)∼P(⋅∣x1(t+1),x3(t)) // freeze x1,x3, only sample x2x3(t+1)∼P(⋅∣x1(t+1),x2(t+1))// freeze x1,x2, only sample x3\\begin{aligned} x_1^{(t+1)} &amp;\\sim P(\\cdot \\mid x_2^{(t)}, x_3^{(t)}) \\quad~~~~~~\\text{// freeze }x_2, x_3\\text{, only sample }x_1\\\\ x_2^{(t+1)} &amp;\\sim P(\\cdot \\mid x_1^{(t+1)}, x_3^{(t)})\\quad~~~\\text{// freeze }x_1, x_3\\text{, only sample }x_2\\\\ x_3^{(t+1)} &amp;\\sim P(\\cdot \\mid x_1^{(t+1)}, x_2^{(t+1)})\\quad\\text{// freeze }x_1, x_2\\text{, only sample }x_3 \\end{aligned} x1(t+1)​x2(t+1)​x3(t+1)​​∼P(⋅∣x2(t)​,x3(t)​) // freeze x2​,x3​, only sample x1​∼P(⋅∣x1(t+1)​,x3(t)​) // freeze x1​,x3​, only sample x2​∼P(⋅∣x1(t+1)​,x2(t+1)​)// freeze x1​,x2​, only sample x3​​ LDA Training Recall that, as mentioned at the beginning of this article, the goal of LDA training is to estimate Θ\\mathbf \\ThetaΘ and Φ\\mathbf \\PhiΦ given the words we see, which requires sampling Z\\mathbf ZZ from P(Z∣W)P(\\mathbf Z \\mid \\mathbf W)P(Z∣W). Let’s do it with Gibbs sampling. As you probably already noticed, the only tricky part is the (*) line, which means “Freezing the values of all the other dimensions, and sampling the current dimension according to their values.” For sampling Z\\mathbf ZZ, this means “freezing the topics of all the other words, and sampling the topic of the current word.” This can be written as: zj∼P(zj=k∣Z¬j,W)z_j \\sim P(z_j=k\\mid \\mathbf{Z}_{\\neg j}, \\mathbf{W}) zj​∼P(zj​=k∣Z¬j​,W) Which means, to sample the topic for the jjj-th word in the corpus (we can imagine “flattening” the corpus by concatenating the words in each of the papers into a long word list, and the index jjj runs through this long word list), we hold the topics for all the other words (Z¬j\\mathbf Z_{\\neg j}Z¬j​) fixed. And if we go a little further, each step of the Gibbs sampling process can be transformed into… P(zj=k∣Z¬j,W)// P(A|B) ∝ P(A, B) when B is observed.∝P(zj=k,wj=t∣Z¬j,W¬j)// Marginalize over θ and φ by taking the integral.=∫θ⃗m∫φ⃗kP(zj=k,wj=t,θ⃗m,φ⃗k∣Z¬j,W¬j)dθ⃗mdφ⃗k// The above probability corresponds to the product of the following events:// 1) θm is sampled from the posterior Dirichlet distribution given Z¬j,W¬j;// 2) Topic k is sampled from θm with probability θmk;// 3) φk is sampled from the posterior Dirichlet distribution given Z¬j,W¬j;// 4) Word t is sampled from φk with probability φkt;=∫θ⃗mθmkP(θ⃗m∣Z¬j,W¬j)dθ⃗m⋅∫φ⃗kφktP(φ⃗k∣Z¬j,W¬j)dφ⃗k// The above two integrals are equivalent to computing the following two expectations:=E[θmk∣Z¬j,W¬j]⋅E[φkt∣Z¬j,W¬j]// Recall that the posteriors of Θ and Φ are Dirichlet distributions,// whose parameters are the sum between the pseudo-counts α, β and the observed counts.// So we count the co-occurrences between the M documents and the K topics NM×K,¬j,// and the co-occurrences between the K topics and the V words NK×V,¬j,// and use them to calculate the expectation of the Dirichlet posterior.// The ¬j subscription means that we exclude word j from the co-occurrences.=αk+Nmk,¬j∑k′(αk′+Nmk′,¬j)⋅βt+Nkt,¬j∑t′(βt′+Nkt′,¬j)(∗∗)\\begin{aligned} &amp;\\quad P(z_j=k\\mid \\mathbf{Z}_{\\neg j}, \\mathbf{W}) \\\\ &amp;\\textcolor{darkgreen}{\\quad\\text{\\texttt{// P(A|B) ∝ P(A, B) when B is observed.}}}\\\\ &amp;\\propto P(z_j=k, w_j=t \\mid \\mathbf{Z}_{\\neg j}, \\mathbf{W}_{\\neg j}) \\\\ &amp;\\textcolor{darkgreen}{\\quad\\text{\\texttt{// Marginalize over θ and φ by taking the integral.}}}\\\\ &amp;= \\int_{\\vec\\theta_m}\\int_{\\vec\\varphi_k} P(z_j=k, w_j=t, \\vec\\theta_m ,\\vec\\varphi_k \\mid \\mathbf{Z}_{\\neg j},\\mathbf{W}_{\\neg j}) d\\vec\\theta_m d\\vec\\varphi_k\\\\ &amp;\\textcolor{darkgreen}{\\quad\\text{\\texttt{// The above probability corresponds to the product of the following events:}}}\\\\ &amp;\\textcolor{darkgreen}{\\quad\\text{\\texttt{// 1) θm is sampled from the posterior Dirichlet distribution given }}\\mathbf{Z}_{\\neg j}, \\mathbf{W}_{\\neg j};}\\\\ &amp;\\textcolor{darkgreen}{\\quad\\text{\\texttt{// 2) Topic k is sampled from θm with probability θmk;}}}\\\\ &amp;\\textcolor{darkgreen}{\\quad\\text{\\texttt{// 3) φk is sampled from the posterior Dirichlet distribution given }}\\mathbf{Z}_{\\neg j}, \\mathbf{W}_{\\neg j};}\\\\ &amp;\\textcolor{darkgreen}{\\quad\\text{\\texttt{// 4) Word t is sampled from φk with probability φkt;}}}\\\\ &amp;= \\int_{\\vec\\theta_m} \\theta_{mk} P(\\vec\\theta_m\\mid \\mathbf{Z}_{\\neg j}, \\mathbf{W}_{\\neg j})d\\vec{\\theta}_m \\cdot \\int_{\\vec\\varphi_k} \\varphi_{kt}P(\\vec\\varphi_{k}\\mid \\mathbf{Z}_{\\neg j},\\mathbf{W}_{\\neg j})d\\vec\\varphi_k\\\\ &amp;\\textcolor{darkgreen}{\\quad\\text{\\texttt{// The above two integrals are equivalent to computing the following two expectations:}}}\\\\ &amp;= \\mathbb{E}[\\theta_{mk} \\mid \\mathbf{Z}_{\\neg j}, \\mathbf{W}_{\\neg j}] \\cdot \\mathbb{E}[\\varphi_{kt}\\mid \\mathbf{Z}_{\\neg j}, \\mathbf{W}_{\\neg j}] \\\\ &amp; \\textcolor{darkgreen}{\\quad\\text{\\texttt{// Recall that the posteriors of Θ and Φ are Dirichlet distributions,}}}\\\\ &amp;\\textcolor{darkgreen}{\\quad\\text{\\texttt{// whose parameters are the sum between the pseudo-counts α, β and the observed counts.}}}\\\\ &amp;\\textcolor{darkgreen}{\\quad\\text{\\texttt{// So we count the co-occurrences between the M documents and the K topics }}\\mathbf{N}_{M\\times K,\\neg j},}\\\\ &amp;\\textcolor{darkgreen}{\\quad\\text{\\texttt{// and the co-occurrences between the K topics and the V words }}\\mathbf{N}_{K\\times V,\\neg j},}\\\\ &amp;\\textcolor{darkgreen}{\\quad \\text{\\texttt{// and use them to calculate the expectation of the Dirichlet posterior.}}}\\\\ &amp;\\textcolor{darkgreen}{\\quad \\text{\\texttt{// The }}\\neg j \\text{\\texttt{ subscription means that we exclude word j from the co-occurrences.}}}\\\\ &amp;= \\dfrac{\\alpha_k+N_{mk, \\neg j}}{\\sum_{k&#x27;} (\\alpha_{k&#x27;}+N_{mk&#x27;,\\neg j})} \\cdot \\dfrac{\\beta_t+N_{kt,\\neg j}}{\\sum_{t&#x27;}(\\beta_{t&#x27;}+N_{kt&#x27;,\\neg j})} \\quad (**) \\end{aligned} ​P(zj​=k∣Z¬j​,W)// P(A|B) ∝ P(A, B) when B is observed.∝P(zj​=k,wj​=t∣Z¬j​,W¬j​)// Marginalize over θ and φ by taking the integral.=∫θm​​∫φ​k​​P(zj​=k,wj​=t,θm​,φ​k​∣Z¬j​,W¬j​)dθm​dφ​k​// The above probability corresponds to the product of the following events:// 1) θm is sampled from the posterior Dirichlet distribution given Z¬j​,W¬j​;// 2) Topic k is sampled from θm with probability θmk;// 3) φk is sampled from the posterior Dirichlet distribution given Z¬j​,W¬j​;// 4) Word t is sampled from φk with probability φkt;=∫θm​​θmk​P(θm​∣Z¬j​,W¬j​)dθm​⋅∫φ​k​​φkt​P(φ​k​∣Z¬j​,W¬j​)dφ​k​// The above two integrals are equivalent to computing the following two expectations:=E[θmk​∣Z¬j​,W¬j​]⋅E[φkt​∣Z¬j​,W¬j​]// Recall that the posteriors of Θ and Φ are Dirichlet distributions,// whose parameters are the sum between the pseudo-counts α, β and the observed counts.// So we count the co-occurrences between the M documents and the K topics NM×K,¬j​,// and the co-occurrences between the K topics and the V words NK×V,¬j​,// and use them to calculate the expectation of the Dirichlet posterior.// The ¬j subscription means that we exclude word j from the co-occurrences.=∑k′​(αk′​+Nmk′,¬j​)αk​+Nmk,¬j​​⋅∑t′​(βt′​+Nkt′,¬j​)βt​+Nkt,¬j​​(∗∗)​ This part is quite dense. In plain English, what this does is: Repeat until converge: Loop through each word, for each word, you count the doc-topic and topic-word co-occurrences without taking into account the current word and its topic; Re-sample the topic of the current word according to (**). In the end, the sequence of the values of the variable Z\\mathbf ZZ generated by Gibbs sampling will be samples of P(Z∣W)P(\\mathbf Z \\mid \\mathbf W)P(Z∣W). For estimating the parameters Θ\\mathbf \\ThetaΘ and Φ\\mathbf\\PhiΦ, we can either take the average between multiple samples, or just approximate with one sample. If we go with one sample, then the estimation for Θ^,Φ^\\hat{\\mathbf\\Theta}, \\hat{\\mathbf{\\Phi}}Θ^,Φ^ will be: θmk=αk+Nmk∑k′(αk′+Nmk′)φkt=βt+Nkt∑t′(βt′+Nkt′)\\begin{aligned} \\theta_{mk} &amp;= \\dfrac{\\alpha_k+N_{mk}}{\\sum_{k&#x27;} (\\alpha_{k&#x27;}+N_{mk&#x27;})} \\\\ \\varphi_{kt} &amp;= \\dfrac{\\beta_t+N_{kt}}{\\sum_{t&#x27;} (\\beta_{t&#x27;}+N_{kt&#x27;})} \\end{aligned} θmk​φkt​​=∑k′​(αk′​+Nmk′​)αk​+Nmk​​=∑t′​(βt′​+Nkt′​)βt​+Nkt​​​ by computing the expectation of their Dirichlet posteriors – combining the pseudo-counts with the observed counts. Here, after the sampling, NmkN_{mk}Nmk​ refers to the count of words with topic k in document m; NktN_{kt}Nkt​ refers to the co-occurrence between word t and topic k in the entire corpus. For Θ\\mathbf\\ThetaΘ, we are combining the pseudo-count α⃗\\vec\\alphaα with the observed doc-topic co-occurrences; For Φ\\mathbf \\PhiΦ, we are combining the pseudo-count β⃗\\vec\\betaβ​ with the observed topic-word co-occurrences. We may also optionally take multiple samples of Z\\mathbf ZZ, use them to estimate Θ\\mathbf \\ThetaΘ and Φ\\mathbf\\PhiΦ separately, and take the average in between. This would take more computational resources, but could provide a less biased estimation. LDA Testing The goal of LDA testing is to estimate the topic distribution θ⃗new\\vec\\theta_{\\text{new}}θnew​ given a new document. The only difference as compared to LDA training, is that the evidence is now W+Wnew\\mathbf W+\\mathbf W_{\\text{new}}W+Wnew​, and all we need to do is to sample the topics for each word in the new document. The Gibbs sampling formula changes to: P(zj=k∣Z¬j,W+Wnew)∝P(zj=k,wj=t∣Z¬j,W,Wnew,¬j)=αk+Nk,¬j∑k′(αk′+Nk′,¬j)⋅βt+Nkt+Nnew,kt,¬j∑t′(βt′+Nkt′+Nnew,kt′,¬j)(∗∗∗)\\begin{aligned} &amp;\\quad P(z_j=k\\mid \\mathbf{Z}_{\\neg j}, \\mathbf{W}+\\mathbf{W}_{\\text {new}}) \\\\ &amp;\\propto P(z_j=k, w_j=t \\mid \\mathbf{Z}_{\\neg j}, \\mathbf{W},\\mathbf{W_{\\text{new}, \\neg j}}) \\\\ &amp;= \\dfrac{\\alpha_k+N_{k, \\neg j}}{\\sum_{k&#x27;} (\\alpha_{k&#x27;}+N_{k&#x27;,\\neg j})} \\cdot \\dfrac{\\beta_t+N_{kt}+N_{\\text{new},kt, \\neg j}}{\\sum_{t&#x27;}(\\beta_{t&#x27;}+N_{kt&#x27;}+N_{\\text{new},kt&#x27;,\\neg j})} \\quad (***) \\end{aligned} ​P(zj​=k∣Z¬j​,W+Wnew​)∝P(zj​=k,wj​=t∣Z¬j​,W,Wnew,¬j​)=∑k′​(αk′​+Nk′,¬j​)αk​+Nk,¬j​​⋅∑t′​(βt′​+Nkt′​+Nnew,kt′,¬j​)βt​+Nkt​+Nnew,kt,¬j​​(∗∗∗)​ Where, Nk,¬jN_{k,\\neg j}Nk,¬j​ refers to the count of words with topic k in the new document (excluding the j-th word), and Nnew,kt,¬jN_{\\text{new},kt,\\neg j}Nnew,kt,¬j​ refers to the co-occurrence of word t with topic k in the new document (excluding the j-th word). The estimation for θ⃗new\\vec\\theta_{\\text{new}}θnew​ would be (where NkN_kNk​ refers to the count of words with topic k in the new document, after we are done with the topic sampling): θnew,k=αk+Nk∑k′(αk′+Nk′)\\theta_{\\text{new},k} = \\dfrac{\\alpha_k+N_k}{\\sum_{k&#x27;}(\\alpha_{k&#x27;}+N_{k&#x27;})} θnew,k​=∑k′​(αk′​+Nk′​)αk​+Nk​​ Example: Topics shift in NeurIPS (19xx-20xx) This section will use a practical example (Papers in NeurIPS) to demonstrate the use of the LDA training and testing. All the codes can be found at https://github.com/mistylight/Understanding_the_LDA_Algorithm. Data We use the NeurIPS papers dataset from this Kaggle competition, which contains the content of the accepted NeurIPS papers ranging between 1987 to 2016. We select the earliest 1000 papers and the latest 1000 papers respectively and train 2 separate LDA models. I’m particularly interested in one question: “How do the topics of NeurIPS shift over the course of ~30 years?” For each paper, I keep the first 200 words from the abstract, and limit the size of the vocabulary to be 10000. I set the number of topics to be K=10, as I expect the NeurIPS conference to have some level of topic diversity but not too much. LDA Training Here, I’ll show the result for the topic-word dices Φ\\mathbf\\PhiΦ as it’s usually of higher interest to us – what topics are present in the corpus? What does each topic look like? The 19xx papers Here are the 10 topics and their top 10 words for the earliest 1000 papers (in other words, they are published in the 20th century). The deeper the color is, the more weight the word has in the corresponding topic: We can roughly see what most of the topics correspond to (note that this interpretation is purely subjective!): Topic 0: training ML models Topic 1: math symbols / circuit design Topic 2: signal processing Topic 3: reinforcement learning Topic 4: neural networks Topic 5: kernel methods Topic 6: speech recognition Topic 7: biological inspiration for neural networks Topic 8: computer vision Topic 9: probability theory / bayesian networks The 20xx papers Here are the 10 topics and their top 10 words for the latest 1000 papers (so they are published around 2016): And we can also try to interpret the meaning of each topic, however this time we can’t explain them super well: Topic 0: training deep neural networks Topic 1: general machine learning Topic 2: (?) Topic 3: generative models (GAN/VAE) Topic 4: neural networks Topic 5: (?) Topic 6: (?) Topic 7: reinforcement learning Topic 8: graph analysis Topic 9: computer vision It’s interesting to see that some topics remain popular after ~30 years, e.g. reinforcement learning and computer vision; Some topics aren’t as popular as they were, e.g. kernel methods; Finally, some topics gain increasing popularity nowadays, e.g. graph analysis. LDA Testing Given a new unseen paper, the LDA testing aims to find its topic distribution. Here, I’ll give an example of using the LDA model trained on the 19xx papers to predict the topics of an unseen 19xx paper: 1234Title: Decomposition of Reinforcement Learning for Admission ControlAbstract: This paper presents predictive gain scheduling, a technique for simplifying reinforcement learning problems by decomposition. Link admissioncontrol of self-similar call traffic is used to demonstrate the technique.The control problem is decomposed into on-line prediction of near-future call arrival rates, and precomputation of policies for Poisson call arrival processes. At decision time, the predictions are used to select among the policies. Simulations show that this technique results in significantly faster learning without any performance loss, compared to a reinforcement learning controller that does not decompose the problem. The predicted topics consist of the following K=10-dim vector (recall that this is the θ⃗new\\vec\\theta_{\\text{new}}θnew​): 1[0.1299, 0.0005, 0.0005, 0.6124, 0.0005, 0.0005, 0.2144, 0.0403, 0.0005, 0.0005] where, each number corresponds to one of the topics. The top topic is Topic 3 (with a weight of 60%+), which aligns with our intuition that this paper is highly relevant to reinforcement learning. Closing Thoughts When I began learning the LDA algorithm in my junior year, I started with the most well-known and comprehensive tutorial in Chinese: LDA Math Gossip (LDA数学八卦) [3]. It is a great tutorial if you are purely interested in math and absolutely not in a hurry to understand and to implement LDA for your project. While I truly appreciate that the tutorial assumes no prior knowledge of stats ML and explains every component of the algorithm with great clarify, I got distracted by the amount of details/proofs that could have been treated as a black box without much harm. It was so hard to keep track of all the variables and their contexts, that when I first read it, I was completely lost. I needed to frequently go back and forth to answer the tons of questions popping into my mind: What is phi? Why are you sampling Z in order to estimate phi? Wait what is Z again? And so on. I believe this struggle isn’t unique and it becomes the motivation to write this article. At the same time, I believe these problems are inevitable if the tutorial is written in a bottom-up fashion. It’s like showing every single pixel at the beginning and gradually zooming out. I try to do the reverse – showing the picture at the beginning and gradually zooming in. The downside is that I can’t go as deep and rigorous into the math as LDA Math Gossip does, which might be unsatisfactory for some readers, but my hope is that it’ll be more friendly to engineers like me, especially to those looking for some intuition about the algorithm, but can’t afford much time to go through all the details. References [1] Latent Dirichlet Allocation. David M. Blei, Andrew Y. Ng, Michael I. Jordan. 2003. https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf [2] Parameter estimation for text analysis. Gregor Heinrich. 2005. http://www.arbylon.net/publications/text-est.pdf [3] LDA数学八卦. 靳志辉. 2013. https://bloglxm.oss-cn-beijing.aliyuncs.com/lda-LDA数学八卦.pdf Appendix You’ve been warned… Why do we want to sample Z In order to compute the expectation for Θ\\mathbf\\ThetaΘ and Φ\\mathbf\\PhiΦ, we need to marginalize upon the joint probability distribution P(Θ,Φ,Z∣W)P(\\mathbf\\Theta, \\mathbf\\Phi, \\mathbf Z \\mid \\mathbf W)P(Θ,Φ,Z∣W): Θ^=EΘ[P(Θ,Φ,Z∣W)]// This is how expectation is defined...=∑Z∫Θ∫ΦΘ⋅P(Θ,Φ,Z∣W)⋅dΘdΦ// P(A,B,C) = P(A|B,C) P(B|C) P(C)=∑Z∫Θ∫ΦΘ⋅P(Θ∣Z,W)⋅P(Φ∣Z,W,Θ)⏟Φ’s estimation only depends on Z and W⋅P(Z∣W)⋅dΘdΦ// Turns out Θ and Φ are conditional independent given Z and W,// because the posterior of Θ (doc-topic dices) can be estimated solely based on Z, // which determines the topic distribution of each document; // The posterior of Φ (topic-word dices) can be solely estimated from Z and W, // which determines the word distribution of each topic.=∑Z∫Θ∫ΦΘ⋅P(Θ∣Z,W)⏟posterior of doc-topic dices⋅P(Φ∣Z,W)⏟posterior of topic-word dices⋅P(Z∣W)⋅dΘdΦ// Recall that the posteriors of Θ and Φ are Dirichlet distributions,// whose parameters are the sum between the pseudo-counts α, β and the observed counts.// So we count the co-occurrences between the M documents and the K topics NM×K,// and the co-occurrences between the K topics and the V words NK×V,// which yields the Dirichlet posterior of Θ and Φ as follows:=∑Z∫Θ∫ΦΘ⋅Dir(Θ∣α⃗+NM×K])⋅Dir(Φ∣β⃗+NK×V)⋅P(Z∣W)⋅dΘdΦ// Just re-arranging, nothing special...=∑ZP(Z∣W)⋅∫ΘΘ⋅Dir(Θ∣α⃗+NM×K])dΘ⏟=EΘ[Dir(Θ∣α⃗+NM×K)]⋅∫ΦDir(Φ∣β⃗+NK×V)dΦ⏟=1// The first integral is the expectation of Θ, and the second integral sums up to 1.=∑ZP(Z∣W)⋅EΘ[Dir(Θ∣α⃗+NM×K)]// The expectation of Dir(⋅) is the (normalized) ratio between its parameters. =∑ZP(Z∣W)⏟intractable⋅[N^mk=αk+Nmk∑k′(αk′+Nmk′)]M×K⏟trivial to compute given Z\\begin{aligned} \\hat{\\mathbf \\Theta} &amp;= \\mathbb{E}_{\\mathbf\\Theta}[P(\\mathbf{\\Theta}, \\mathbf{\\Phi}, \\mathbf{Z} \\mid \\mathbf{W} )] \\\\ &amp;\\quad\\text{\\texttt{// This is how expectation is defined...}}\\\\ &amp;=\\sum_{\\mathbf{Z}}\\int_{\\mathbf{\\Theta}}\\int_{\\mathbf\\Phi}\\mathbf{\\Theta} \\cdot P(\\mathbf{\\Theta}, \\mathbf{\\Phi}, \\mathbf{Z} \\mid \\mathbf{W} ) \\cdot d\\mathbf{\\Theta}d\\mathbf{\\Phi} \\\\ &amp;\\quad\\text{\\texttt{// P(A,B,C) = P(A|B,C) P(B|C) P(C)}}\\\\ &amp;=\\sum_{\\mathbf{Z}}\\int_{\\mathbf{\\Theta}}\\int_{\\mathbf\\Phi} \\mathbf{\\Theta} \\cdot P(\\mathbf{\\Theta}\\mid \\mathbf{Z},\\mathbf{W})\\cdot \\underbrace{P(\\mathbf{\\Phi}\\mid \\mathbf{Z},\\mathbf{W},\\sout{\\mathbf{\\Theta}})}_{\\mathbf{\\Phi}\\text{&#x27;s estimation only depends on }\\mathbf{Z} \\text{ and }\\mathbf{W}}\\cdot P(\\mathbf{Z} \\mid \\mathbf{W} )\\cdot d\\mathbf{\\Theta}d\\mathbf{\\Phi}\\\\ &amp;\\quad\\text{\\texttt{// Turns out Θ and Φ are conditional independent given Z and W,}}\\\\ &amp;\\quad\\text{\\texttt{// because the posterior of Θ (doc-topic dices) can be estimated solely based on Z, }}\\\\ &amp;\\quad\\text{\\texttt{// which determines the topic distribution of each document; }}\\\\ &amp;\\quad\\text{\\texttt{// The posterior of Φ (topic-word dices) can be solely estimated from Z and W, }}\\\\ &amp;\\quad\\text{\\texttt{// which determines the word distribution of each topic.}}\\\\ &amp;=\\sum_{\\mathbf{Z}}\\int_{\\mathbf{\\Theta}}\\int_{\\mathbf\\Phi} \\mathbf{\\Theta} \\cdot \\underbrace{P(\\mathbf{\\Theta}\\mid \\mathbf{Z},\\mathbf{W})}_{\\text{posterior of doc-topic dices}}\\cdot \\underbrace{P(\\mathbf{\\Phi}\\mid \\mathbf{Z},\\mathbf{W})}_{\\text{posterior of topic-word dices}}\\cdot P(\\mathbf{Z} \\mid \\mathbf{W})\\cdot d\\mathbf{\\Theta}d\\mathbf{\\Phi}\\\\ &amp;\\quad\\text{\\texttt{// Recall that the posteriors of Θ and Φ are Dirichlet distributions,}}\\\\ &amp;\\quad\\text{\\texttt{// whose parameters are the sum between the pseudo-counts α, β and the observed counts.}}\\\\ &amp;\\quad\\text{\\texttt{// So we count the co-occurrences between the M documents and the K topics }}\\mathbf{N}_{M\\times K},\\\\ &amp;\\quad\\text{\\texttt{// and the co-occurrences between the K topics and the V words }}\\mathbf{N}_{K\\times V},\\\\ &amp;\\quad\\text{\\texttt{// which yields the Dirichlet posterior of Θ and Φ as follows:}}\\\\ &amp;= \\sum_{\\mathbf{Z}}\\int_{\\mathbf{\\Theta}}\\int_{\\mathbf\\Phi} \\mathbf{\\Theta} \\cdot \\text{Dir}(\\mathbf{\\Theta}\\mid \\vec\\alpha+\\mathbf{N}_{M\\times K}])\\cdot \\text{Dir}(\\mathbf{\\Phi}\\mid \\vec\\beta+\\mathbf{N}_{K\\times V})\\cdot P(\\mathbf{Z} \\mid \\mathbf{W})\\cdot d\\mathbf{\\Theta}d\\mathbf{\\Phi} \\\\ &amp;\\quad\\text{\\texttt{// Just re-arranging, nothing special...}}\\\\ &amp;= \\sum_{\\mathbf{Z}}P(\\mathbf{Z} \\mid \\mathbf{W})\\cdot \\underbrace{\\int_{\\mathbf{\\Theta}}\\mathbf{\\Theta} \\cdot \\text{Dir}(\\mathbf{\\Theta}\\mid \\vec\\alpha+\\mathbf{N}_{M\\times K}])d\\mathbf{\\Theta}}_{=\\mathbb{E}_{\\mathbf\\Theta}[\\text{Dir}(\\mathbf\\Theta\\mid\\vec\\alpha+\\mathbf{N}_{M\\times K})]}\\cdot\\underbrace{\\int_{\\mathbf\\Phi} \\text{Dir}(\\mathbf{\\Phi}\\mid \\vec\\beta+\\mathbf{N}_{K\\times V}) d\\mathbf{\\Phi} }_{=1}\\\\ &amp;\\quad\\text{\\texttt{// The first integral is the expectation of }}\\mathbf{\\Theta}\\text{\\texttt{, and the second integral sums up to 1.}}\\\\ &amp;= \\sum_{\\mathbf{Z}}P(\\mathbf{Z} \\mid \\mathbf{W})\\cdot \\mathbb{E}_{\\mathbf\\Theta}[\\text{Dir}(\\mathbf\\Theta\\mid\\vec\\alpha+\\mathbf{N}_{M\\times K})]\\\\ &amp;\\quad\\text{\\texttt{// The expectation of }} \\text{Dir}(\\cdot)\\text{\\texttt{ is the (normalized) ratio between its parameters. }}\\\\ &amp;= \\underbrace{\\sum_{\\mathbf{Z}}P(\\mathbf{Z} \\mid \\mathbf{W})}_{\\text{intractable}}\\cdot \\underbrace{\\left[\\hat{N}_{mk}=\\dfrac{\\alpha_k+N_{mk}}{\\sum_{k&#x27;} (\\alpha_{k&#x27;}+N_{mk&#x27;})}\\right]_{M\\times K}}_{\\text{trivial to compute given }\\mathbf{Z}} \\end{aligned} Θ^​=EΘ​[P(Θ,Φ,Z∣W)]// This is how expectation is defined...=Z∑​∫Θ​∫Φ​Θ⋅P(Θ,Φ,Z∣W)⋅dΘdΦ// P(A,B,C) = P(A|B,C) P(B|C) P(C)=Z∑​∫Θ​∫Φ​Θ⋅P(Θ∣Z,W)⋅Φ’s estimation only depends on Z and WP(Φ∣Z,W,Θ)​​⋅P(Z∣W)⋅dΘdΦ// Turns out Θ and Φ are conditional independent given Z and W,// because the posterior of Θ (doc-topic dices) can be estimated solely based on Z, // which determines the topic distribution of each document; // The posterior of Φ (topic-word dices) can be solely estimated from Z and W, // which determines the word distribution of each topic.=Z∑​∫Θ​∫Φ​Θ⋅posterior of doc-topic dicesP(Θ∣Z,W)​​⋅posterior of topic-word dicesP(Φ∣Z,W)​​⋅P(Z∣W)⋅dΘdΦ// Recall that the posteriors of Θ and Φ are Dirichlet distributions,// whose parameters are the sum between the pseudo-counts α, β and the observed counts.// So we count the co-occurrences between the M documents and the K topics NM×K​,// and the co-occurrences between the K topics and the V words NK×V​,// which yields the Dirichlet posterior of Θ and Φ as follows:=Z∑​∫Θ​∫Φ​Θ⋅Dir(Θ∣α+NM×K​])⋅Dir(Φ∣β​+NK×V​)⋅P(Z∣W)⋅dΘdΦ// Just re-arranging, nothing special...=Z∑​P(Z∣W)⋅=EΘ​[Dir(Θ∣α+NM×K​)]∫Θ​Θ⋅Dir(Θ∣α+NM×K​])dΘ​​⋅=1∫Φ​Dir(Φ∣β​+NK×V​)dΦ​​// The first integral is the expectation of Θ, and the second integral sums up to 1.=Z∑​P(Z∣W)⋅EΘ​[Dir(Θ∣α+NM×K​)]// The expectation of Dir(⋅) is the (normalized) ratio between its parameters. =intractableZ∑​P(Z∣W)​​⋅trivial to compute given Z[N^mk​=∑k′​(αk′​+Nmk′​)αk​+Nmk​​]M×K​​​​ The integral w.r.t. Θ,Φ\\mathbf\\Theta, \\mathbf\\PhiΘ,Φ are feasible to compute once we have Z\\mathbf ZZ – since we are trying to find the posterior of doc-topic and topic-word dices, and that we’ve assumed their priors to be Dirichlet, we can just incorporate our observation – the occurrences of topics in a document and words with a topic, with the pseudo-count – α⃗\\vec\\alphaα and β⃗\\vec\\betaβ​, to get the parameters of the Dirichlet posterior. And since we know how to compute the expectation of Dirichlet distributions (which is itself, an integral in the same form), these two parts aren’t of much concern. The real trouble is the summation over all possible values of Z\\mathbf ZZ! Since Z\\mathbf{Z}Z is a (M×N)(M \\times N)(M×N)-dim discrete variable (remember it’s the topic of each word) with KKK possible values for each dimension, an exhaustive enumeration would be intractable. Is there any chance we can bypass the exhaustive enumeration over Z\\mathbf ZZ? The tool we use the intractability, is “sampling”. In computer science, a common practice to model complicated distributions, is to generate samples of it. That is to say, if you are interested in some very complicated distribution p(x⃗)p(\\vec{x})p(x), then you probably want to generate a large amount of samples x⃗i∼p(x⃗)\\vec x_i \\sim p(\\vec x)xi​∼p(x). Now if you wish to compute some attributes of the distribution, e.g. its expectation E[p(x⃗)]\\mathbb{E}[p(\\vec x)]E[p(x)], you can simply take the average of your samples as a reasonable approximation. Otherwise, we’d need to take an integral to calculate the expectation precisely, which is typically intractable in practice. Given nnn samples [Z1,Z2,...,Zn][\\mathbf Z_1, \\mathbf Z_2, ..., \\mathbf Z_n][Z1​,Z2​,...,Zn​] (or, sometimes if our computation resources are really limited, we may even just go with one example, which means n=1n=1n=1) drawn from P(Z∣W)P(\\mathbf{Z}\\mid\\mathbf{W})P(Z∣W), we’d be able to approximate ∑ZP(Z∣W)f(Z)≈1n∑i=1nf(Zi)\\sum_{\\mathbf Z} P(\\mathbf Z\\mid \\mathbf W)f(\\mathbf Z)\\approx \\dfrac{1}{n} \\sum_{i=1}^n f(\\mathbf Z_i) Z∑​P(Z∣W)f(Z)≈n1​i=1∑n​f(Zi​) here, f(Z)f(\\mathbf Z)f(Z) can be any function of Z\\mathbf ZZ. In our calculation of EΘ[P(Θ∣W)]\\mathbb{E}_{\\mathbf \\Theta}[P(\\mathbf\\Theta\\mid \\mathbf W)]EΘ​[P(Θ∣W)] above, we have f(Z)f(\\mathbf Z)f(Z) equal to αk+Nmk∑k′(αk′+Nmk′)\\dfrac{\\alpha_k+N_{mk}}{\\sum_{k&#x27;} (\\alpha_{k&#x27;}+N_{mk&#x27;})}∑k′​(αk′​+Nmk′​)αk​+Nmk​​. So the next question becomes how do we generate samples from a distribution, which goes back to the Gibbs sampling section 😃","categories":[{"name":"ML","slug":"ML","permalink":"https://mistylight.github.io/categories/ML/"},{"name":"Algorithms","slug":"ML/Algorithms","permalink":"https://mistylight.github.io/categories/ML/Algorithms/"}],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"https://mistylight.github.io/tags/tutorial/"},{"name":"understanding-X","slug":"understanding-X","permalink":"https://mistylight.github.io/tags/understanding-X/"},{"name":"probability-theory","slug":"probability-theory","permalink":"https://mistylight.github.io/tags/probability-theory/"},{"name":"bayesian-network","slug":"bayesian-network","permalink":"https://mistylight.github.io/tags/bayesian-network/"},{"name":"simple-code-heavy-math","slug":"simple-code-heavy-math","permalink":"https://mistylight.github.io/tags/simple-code-heavy-math/"},{"name":"LDA","slug":"LDA","permalink":"https://mistylight.github.io/tags/LDA/"},{"name":"topic-modeling","slug":"topic-modeling","permalink":"https://mistylight.github.io/tags/topic-modeling/"}]}]}